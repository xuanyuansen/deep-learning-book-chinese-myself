##第十四章：Auto-encoders 自动编码器自动编码器是被训练用于重现输入到输出的神经网络。在内部，它有一个描述用于表示输入的编码的隐含层h。该网络可以被看作是由两部分组成：一个编码器函数h = F（X ），和用于重建的解码函数R = G （H）。该架构如图14.1 所示。如果自动编码器成功地全局简单学习到g(f(x)) = x ，那么它并不是特别有用。相反，自动编码器被设计成不能学习到完美的复制。通常他们工作的方式被限制为仅允许近似地复制，且仅复制可以重构训练数据的输入。因为模型被强制地优先考虑哪方面的输入需要复制，所以它往往能够学习到数据的有用特征。
现代自动编码器已经泛化了编码器和解码器的定义，而不仅仅是确定随机映射函数pencoder(h | x) 和pdecoder(x | h)。
自动编码器的思想一直是神经网络持续几十年的历史的一部分。历史上，自动编码器被用于降维和特征学习。近年来，自动编码器和隐变量模型的理论联系将自动编码器推到了生成模型的前沿，详见第20章。自动编码器可以被看作是前馈网络的特殊情况，可使用所有相同的技术来训练，通常是基于minibatch反向传播算法来计算梯度的下降。不同于普通的前馈网络，自动编码器也可使用再循环recirculation (Hinton and McClelland, 1988) 来训练，这是一个基于比较神经网络输入的激励和重建这个激励的算法。再循环被视为比反向传播具有更多的生物合理性，但很少用于机器学习应用程序。 
图14.1 一般自动编码器的构造，通过内部表征或称为编码h完成从输入x到输出r（也成称为重建）的映射。自动编码器有两个部分，编码器f（从x到h的映射）和解码器g（从h到r的映射）。###14.1	 非完备的自动编码器复制输入到输出听起来好像是没有用的，但是我们通常对解码器的输出不感兴趣。相反地，我们希望训练自动编码器完成输入的复制任务会使得隐含层h获得有用的信息。从自编码获器得有用的特性的一种方法是限制隐含层h的维度小于输入x的维度。一个自动编码器的编码维度小于输入维度被称为非完备的。学习非完备的表达强制自动编码器去捕捉训练数据中最显著的特征。学习过程可以简单描述为最小化如下的损失函数。
L(x, g(f (x)))（14.1） 
其中L是惩罚g(f(x)) 偏离x的损失函数，例如均方误差。
当解码器是线性的且L是均方误差时，一个非完备的自动编码器能够学习如PCA算法一样的子空间。在这种情况下，训练执行复制任务的自动解码器从侧面学习了训练数据的主空间。
因此，包含非线性编码器f和非线性解码器g的自动编码器能够学习更强大的PCA的非线性推广。然而不幸的是，如果编码器和解码器的表达能力太强，自动编码器会学习执行复制的任务，而不是从数据的分布中提取有用信息。在实践中如上特定情况即使不发生，但是它清楚地说明，如果允许自动编码器的容量太大，训练用于进行复制任务的自动编码器可能无法学习数据集的任何有用信息。
###14.2	正则化自编码器（Regularized Auto-encoders）非完备自编码器的编码维度小于输入维度，可以学习到数据分布中最显著的特征。我们已经见过编码器和解码器表达能力太强，从而造成自动编码器未能学到任何有用信息的例子。
如果隐含层编码的维数与输入维度相同，或者隐含层编码的维度大于输入的维度，也会发生类似的问题。在这些情况下，即使是线性编码器和线性解码器也只学习到复制输入到输出，而学习不到任何关于数据分布有用信息。
理想情况下，根据要建模的分布的复杂性选择编码器的维度及编码器和解码器的表达能力，可以成功训练任何结构的自编码器。正则化的自编码器提供了这样做的能力。与通过编码器和解码器为浅层且编码的尺寸较小，来限制模型的表达能力不同，正则化的自编码器使用损失函数来鼓励模型获得除了拷贝输入到输出之外的能力。这些其他性质包括表达的稀疏性，表征的小梯度，对于噪音和输入缺失的鲁棒性。正则化的自编码器可能是非线性和过分完备的，即使模型的表达能力强到能够学习严格恒等的函数，但是仍然能够从数据分布中学习到有用的信息。
除了这里介绍的自然被解释为正则化的自动编码器的方法，几乎所有含有推理步骤的带隐变量的生成模型（利用输入计算隐变量表示）都可以被看做是自编码器的一种特殊形式。两种生成模型方法强调了与自编码器的联系，他们是Helmholtz机(Hinton et al., 1995b) 的衍生方法，例如variational autoencoder（VAE）（20.10.3章节）和随机生成网络（generative stochastic networks）（20.12章节）。这类模型自然学习到较高的表达能力和输入的过完备编码，不需要正则化对这些自编码器起作用。他们的编码自然是有用的，因为模型被训练用于近似训练数据的最大化概率，而不是复制输入到输出。
####14.2.1	稀疏自编码器（Sparse Auto-encoders ）稀疏自编码器也是编码器，只是其训练过程在重建误差之外增加了针对隐含层h的稀疏惩罚项Ω(h) 。

L(x, g(f (x))) + Ω(h) （14.2）
其中g(h) 是解码器的输出，一般地我们有h = f(x) ，即编码器的输出。
通常稀疏编码器被用于学习用于其他任务的特征。自编码器被正则化稀疏后必须要响应其对应训练数据的特殊统计特性，而不仅仅是恒等的函数。以这样的方式，加入稀疏惩罚来训练执行复制任务，会生成能副作用地学到有用特征的模型。
我们可以把惩罚项Ω(h)简单地想成加入前馈神经网络的正则项，这个神经网络的基本任务是复制输入到输出（非监督学习目标），取决于这些稀疏特征，还可能执行一些有监督的任务（监督学习目标）。这个正则化方法没有一个直接的贝叶斯解释，不同于其他正则化方法，例如权重衰减。
如5.6.1节所描述，带权重衰减和其它正则项惩罚的训练可以被解释为贝叶斯推理过程的最大后验概率近似，正则项惩罚相当于模型参数的先验概率分布。从这个角度，正则化的最大似然对应于最大化p(θ | x) ，等价于最大化log p(x | θ) + log p(θ) ，其中log p(x | θ) 通常是对数似然项，logp(θ) 是参数的对数先验，考虑了特定θ值的偏好。这种观点在5.6节有描述。正则化的自动编码器违背了这样的解释，因为正则项取决于数据，所以这样的定义是不符合先验本身字面意义的。但是我们仍然可以认为这些正则项隐含地表达了函数的偏好。我们应该把整个稀疏的自动编码框架当做是近似训练包含隐变量的最大似然概率模型，而不是把稀疏惩罚当做复制任务的惩罚项。
假设我们有一个包含显变量x和隐变量h的模型，且包含明确的联合分布pmodel(X,H)= pmodel(H) pmodel(X|H)。我们用pmodel(H) 表示模型隐变量的先验分布，表示模型在X可见前的置信度。这与我们先前使用单词“prior”的方式不同，分布p(θ) 指的是在训练数据可见之前我们对模型参数的置信度。对数似然可以被分解为，
log p model (x) = log X pmodel (h, x) （14.3）
我们可以将自动编码器理解为使用一个最可能h值的点估计来近似这个和。这和稀疏编码生成模型很像，但是增加了h来表示参数编码的输出，而不是优化最可能h的结果。这样看来，选定h，我们可以最大化，
log p model ( h , x ) = log p model ( h ) + log p model ( x | h )（14.4）
log p model ( h , x )可以是稀疏的。例如拉普拉斯先验（Laplace prior ）。
对应于稀疏惩罚的绝对值。将对数先验表达成惩罚的绝对值，我们可以得到。
其中常数项仅依赖于λ而不是H。我们通常将λ作为超参数而丢弃常数项，因为它不会影响该参数的学习。其他先验如t分布也会引入稀疏性。从这个角度看pmodel （H）由近似最大似然学习导致的稀疏性，稀疏惩罚根本不是正则项。它只是在隐变量存在下模型分布导致的结果。这种观点提供了训练自动编码器的另一个动机：这是一种近似训练生成模型的方法。这也从另一个角度解释了为什么自动编码器学习到的特征是有用的：他们可以描述解释输入的隐变量。
稀疏自动编码器（Ranzato et al., 2007a, 2008）的早期工作探讨了各种形式的稀疏性，并提出了当应用最大似然于无向图模型时，正则惩罚项和对数项Z之间产生联系。其思想是最小化Z的对数来防止概率模型在任何地方都有较高的概率，自动编码器的稀疏性防止其在任何地方都有较低的重建误差。在这种情况下，这种联系是基于普遍原理的直觉，而不是数学表达。将稀疏惩罚项对应于概率有向图pmodel (h)pmodel(x | h) 中的pmodel (h)来理解更具有数学直观性。
####14.2.2	Denoising Autoencoders（去噪自动编码器）与为损失函数增加惩罚项Ω不同，我们可以通过改变损失函数中的重建误差项来学习有用的性质。一般地，自动编码器最小化一些形式的函数，
L(x, g(f (x))) 
其中L是惩罚g(f(x)) 与X相异程度的损失函数，例如它们之间差的L2范数。这鼓励g ◦ f仅仅学习到恒等的函数，如果它们具有这样的能力。
去噪自动编码器（DAE）相反最小化
L ( x , g ( f ( ̃x ) ) ) 
其中x ̃ 是x的拷贝，但是混杂了一些形式的噪音。所以DAE必须学会消除这样的混杂而不仅仅是复制其输入。
去噪训练过程强制f和g隐式地学习pdata(x) 的结构，如Alain and Bengio (2013)和Bengio et al. (2013c)的研究所示。去噪自动编码器因此提供了另外一个例子，展示了最小化重建误差可以使得有用的性质以副产品的形式产生。他们也展示了完备且表达能力强的模型可以被当成自动编码器使用，只需小心使用避免他们学习到恒等函数。去噪自动编码器在章节14.5中有更详细的描述。1.2.3	Regularizing by Penalizing Derivatives （惩罚偏导数的正则化）另一种正则化自动编码器的方式是使用形如稀疏自动编码器中的惩罚项Ω ，即L(x, g(f (x))) + Ω(h, x) 但是Ω的形式不同。Ω(h, x) = λ X ||∇xhi ||2  这强制了模型学习到在X变化很大时不会剧烈波动的函数。因为这个惩罚项是仅仅应用于训练样本的，它强制了自动编码器学到可以捕捉有关训练数据分布信息的特征。
如此正则化的自动编码器被称为压缩自动编码器（CAE）。这种方法与很多研究有理论联系，例如去噪自动编码器（denoising autoencoders），流形学习（manifold learning）和概率建模（probabilistic modeling）。CAE在章节14.7中有更多细节描述。
###14.3	表达能力，层的尺寸和深度（Representational Power, Layer Size and Depth）自动编码器通常仅使用一个编码层和一个解码层来训练。然而这并不是必须条件。实际上，使用深度的编码器和解码器有很多优点。
如章节6.4.1所述，前馈神经网络中的深度有很多优势。因为自动编码器是前馈神经网络，所以这些优势也适用于自动编码器。此外，编码器和解码器本身都是一个前馈网络，所以自动编码器的各个部分都可以单独从深度中获益。
不同于寻常的深度的一个主要优点是，通用逼近定理保证了至少包含一个隐含层的前馈神经网络能够以任意的精度近似表示任何函数（一大类），只要其包含足够的隐单元。这意味着只含有一个隐层的自动编码器能够任意好地表达数据维度的恒等函数。然而从数据到编码的映射是很浅层的。这意味着我们不能强制地添加任意约束，例如编码必须是稀疏的。一个深层的自动编码器，其中编码层自身至少包含一个额外的隐含层，可以任意好地近似从输入到编码的任何映射，只要其包含足够多的隐单元。
深度可以成倍减少表示某些函数表达的计算成本。深度也可以指数地降低学习一些功能所需要的训练数据量。前馈神经网络中深度的优势请回顾章节6.4.1。
从实验结果看来，深层自动编码器相比于浅层或线性自动编码器，能够获得更好的压缩效果（Hinton and Salakhutdinov, 2006）。
训练深层自动编码器的一个普遍策略是，通过训练堆叠的浅层自动编码器来逐层贪心地预训练深层结构，所以我们经常要面对浅层编码器，即使我们的终极目标是训练深层的自动编码器。
###14.4	随机编码器和解码器（Stochastic Encoders and Decoders）自动编码器仅仅是前馈神经网络。用于传统前馈神经网络的损失函数和输出单元类型，同样适用于自动编码器。
如章节6.2.2.4所示，用于设计输出单元和前馈网络的损失函数的一般策略是定义一个输出分布P（Y | X），和最小化负对数似然− log p(y | x)。在这样的设定中，Y是目标向量，例如类别标签。
以自动编码器为例，x现在既是目标也是输入。然而，我们仍然可以使用同样的机制。给定隐含编码h，我们可以将解码器理解为提供条件分布pdecoder (x | h)。然后，我们可以通过最小化− log pdecoder(x | h) 来训练自编码器。此损失函数的确切形式会随着pdecoder的形式改变。与传统的前馈网络类似，如果x是实数，我们通常使用线性输出单元来参数化高斯分布的平均值，在这种情况下，负对数似然产生一个均方误差准则。同样，二进制x值对应于一个伯努利分布，其参数通过S形输出单元给出，离散x值对应于SOFTMAX分布，等等。通常情况下，输出变量在给定H的情况下被视为是条件独立，所以评估这个概率分布的成本并不高，但是一些技术允许在输出有相关性的情况下进行可控的建模，例如混合密度输出。 
图14.2：一个随机自动编码器的结构，其中编码器和解码器并不是简单的函数而是包含了噪声注入，这意味着他们的输出可以被视为从某个分布的采用，编码器对应于pencoder(h | x) ，而解码器对应于pdecoder(x | h) 。
为了更好地区别我们以前见过的前馈网络，我们可以推广编码函数f(x)的概念到编码分布pencoder(h | x) ，如图14.2所示。
任何包含隐变量的模型pmodel (h, x)都定义了一个随机编码器。
pencoder(h | x) = pmodel(h | x) 
和一个随机解码器，
pdecoder(x | h) = pmodel(x | h) 
在一般情况下，编码器和解码器的条件分布未必与一个独特的联合分布pmodel(X,H)一致。Alain等(2015)研究表明，训练编码器和解码作为去噪自编码器时会使得他们渐进地一致（在足够的表达能力和训练样本条件下）。
###14.5	去噪自动编码器(Denoising Autoencoders)去噪自动编码器（DAE）是接收被混淆的数据点作为输入，且被训练来预测输出原始未被混淆的数据点的自动编码器。
DAE的训练过程如图14.3所示。我们引入一个混淆过程，它代表了给定原始数据X情况下，混淆样本的条件分布。那么自动编码器将根据训练数据对pairs (x, ̃x) 的估计，来学习重建分布preconstruct(x | ̃x)。其步骤是：
*	1、从训练数据中采样得到训练样本。*	2、根据分布C( ̃x | x = x) 从训练样本中采样得到混淆的x ̃ 。*	3、使用( x,x ̃) 作为训练样本来估计自动编码器的重建分布preconstruct(x | ̃x) = pdecoder(x | h) ，其中h是编码器f (x ̃)的输出，而解码器一般定义为g(h) 。
通常情况下，我们可以简单地对负对数似然− log pdecoder(x | h) 执行基于梯度的最小化近似（例如minibatch梯度下降）。只要编码器是确定性的，去噪自编码就相当于一个前馈网络，并可使用与训练任何其它前馈网络完全相同的技术来训练。
因此，我们可以将DAE看成是以如下形式进行随机梯度下降： 
其中是训练（数据）的分布。 图14.4 去噪自编码被训练用于将一个混淆的数据点x映射回原始数据点x 。我们举例说明，训练样本x用红叉表示，该样本在用黑色粗线所示的低维流形附近。我们用灰色圆圈来表示等概率的混淆过程C ( ̃x | x) 。灰色箭头表明一个训练样本是如何通过混淆过程转变为一个样本。当去噪编码器的训练目标是最小化均方误差||g(f( ̃x))−x||2 时，重建函数g(f( ̃x)) 估计的是Ex,x ̃∼pdata(x)C( ̃x|x)[x |x ̃] 。向量g(f( ̃x)) −x ̃ 大致指向流形上最近的点，这是因为g(f( ̃x))估计的是最可能复现x ̃的原始x点的质量中心。因此，自动编码器学习到的是用绿色箭头表示的矢量域g(f(x)) − x。该矢量域估计得分∇xlogpdata(x) 的上限是重建误差均方根的乘积。
####14.5.1	分数估计（Estimating the Score）得分匹配（Hyvärinen, 2005）是最大似然之外的另一个选择。它提供了概率分布的一个连续估计，鼓励模型在每个训练点x都具有与其数据分布相同的得分。在这种情况下，得分是一个特定的梯度场： 得分匹配将在章节18.4中更深入的讨论。就目前关于自动编码器的讨论，这些了解足以理解学习log pdata 的梯度场就是学习pdata 自身结构的一种方式。
DAE的一个重要特性是，他们的训练标准（包含高斯条件分布p(x | h))）使得自动编码器可以学习到一个向量场(g(f(x)) − x) 来估计数据分布的得分。如图14.4所示。
去噪训练一种使用高斯噪声和均方误差作为重建误差的特殊形式的自动编码器（S形隐单元，线性重构单元），等价于(Vincent, 2011)训练带有高斯显单元的特殊形式的概率无向图模型RBM。RBM这种模型将在章节20.5.1中有更详细的描述；就目前的讨论，了解该模型提供了一个明确的pmodel (x;θ) 就足够了。当RBM使用去噪得分匹配（denoising score matching (Kingma and LeCun, 2010)）来训练时，其学习算法等价于去噪训练其对应的自动编码器。当噪生水平固定时，正规化得分匹配就不是一个连续的估计；它相反会恢复一个模糊版本的分布。然而，如果噪声水平接近0而样本数量接近无穷大时，那么将恢复连续的分布。去噪得分匹配（Denoising score matching）将在章节18.5有更详细的讨论。
自动编码器和RBM也存在其他的联系。对RBM使用得分匹配得到的损失函数，等价于包含类似CAE的收缩刑罚则项的正则项的重建误差(Swersky et al., 2011)。Bengio和Delalleau (2009)展示了自动编码器梯度提供了RBM对比散度训练的近似。
对于连续值x，包含高斯混淆和重建分布的去噪准则可以产生分数的估计，而这个估计适用于生成编码器和解码器的参数。这意味着一般通用的编码器——解码器架构可以用于估计得分，其中训练使用均方误差准则，
||g(f (x ̃)) − x|| 2 混淆过程，
C( ̃x = x ̃|x) = N(x ̃;μ = x,Σ = σ2I) 
和噪声方差σ 2 
图14.5证明了其工作过程。
图14.5：一般情况下，并不能保证该重构函数g(f (x))减去输入x对应于任何函数的梯度，更不用说得分。这就是为什么早期结果(Vincent, 2011)专注于特定的参数化，其中g(f(x)) − x可通过取另一个函数的导数来获得。Kamyshanska and Memisevic (2015)通过识别一个浅层自动编码器族推广了(Vincent, 2011)的结果，这样g (f( x)) − x可以对应这一族所有成员的得分。
到目前为止，我们已经介绍的仅仅是去噪自动编码器如何学会表达一个概率分布。更一般地，人们可能希望自动编码器作为生成模型使用，并从该分布中采样。这将在之后的章节20.11中描述。
###14.5.1.1	历史观点使用MLP来进行去噪的想法可以追溯至LeCun（1987）和Gallinari（1987）等人的工作。Behnke同样使用循环网络来做图像去噪。去噪自动编码器在某种意义上只是训练MLP来降噪。然而，“去噪自动编码器”指的是一种不仅会去学习对输入进行去噪的模型，同时可以在学习降噪的过程中学习到数据的内部结构。这个想法来得很晚（Vincent等，2008，2010）。学习到的表达然后可以用于预训练一个深层的无监督网络或者有监督网络。与稀疏自动编码器，稀疏编码，收缩自动编码器和其他正则化的自动编码器一样，去噪自动编码器的动机是允许，在防止编码器和解码器学习到无用的恒等方程的情况下，同时学习到一个表达能力非常强的编码器。
在引入现代DAE之前，Inayoshi和Kurita（2005）使用一些同样的方法探索了其中一些相同的目标。他们的做法是在有监督的MLP的隐层注入噪声，目标是通过引入重建误差和注入噪声提高泛化能力，在有监督的目标之外最大限度地减少重建误差。然而，他们的方法是基于线性编码器的，无法学习到和现代DAE一样强大的函数族。
###14.6	基于自动编码器的流形学习（Learning Manifolds with Autoencoders）像许多其他的机器学习算法一样， 自动编码器利用了这样的想法，数据集中围绕一个低维流形或者类似流形的小集合，如章节5.11.3所述 。一些机器学习算法可以利用这样的想法仅仅是因为他们学到了一个能在该流形表现正确的函数，如果给其一个偏离流形的输入可能表现异常。自动编码器借鉴并延伸了该想法，旨在学习到流形的结构。为了理解自动编码器是如何做到这样的，我们需要展示流形的一些重要特性。
流形的一个重要特性是其切平面集合。在d维流形上的点x，切平面是由流形允许变化方向的本地展开基向量d给出。如图14. 6所示，这些地方的方向指定了x如何无穷更改而同时停留在流形上。
所有自动编码器的训练过程都包含两股力量间的妥协。
*	1、从训练样本x中学习到一个表示h，所以x可以从h通过解码器近似恢复。x是从训练数据得出的事实是重要的，因为这意味着在自编码不需要成功重建不属于该数据生成分布下的可能输入。*	2、满足约束或者正规化惩罚。这可以是限制自动编码器表达能力的结构约束，或者是加入重建损失的正则项。这些技术一般倾向于对输入不敏感的解决方案。

显然，单独强调某一方是没用的，复制输入到输出对其自身是没用的，忽略输入也是没用的。相反，这两个强制约束同时存在就是有用的，因为他们强制隐层的表达去捕捉关于数据生成分布结构的信息。重要的原则是，自动编码器只能表示重建所需要的训练样本的变量。如果该数据生成分布集中靠近低维流形，这产生的表达隐式地捕捉了该流形的局部坐标：仅正切于流形的x周围的变量需要响应H=F（X）的变化。所以编码器学习到了从输入空间x到表征空间的映射，这个映射只对沿着流形方向的变化敏感，而对正交于流形的变化不敏感。
图14.7展示了一个一维的例子，这表明通过使重建函数对数据点周围的输入扰动不敏感，我们可以恢复流形的结构。
要了解为什么自动编码器对于流形学习是有用的，将其与其他方法对比是非常有启发的。最常见的学习到用于表征一个流形的就是在该流形之上（接近该流形）的数据点的表示。一个特定样本的表征也可以称为其“嵌入”（embedding）。这通常由一个低维的向量表示，比其“周围”空间有更低的维度，而“周围”空间中的流形是一个低维子集。有些算法（非参数流形学习算法，下面将讨论）直接学习每个训练样本的“嵌入”，而其它算法学习更一般的映射，有时也被称为一个编码器，或表征函数，将周围空间（输入空间）中的任意点映射到它的“嵌入”。
流形学习大多数聚焦于试图捕捉这些流形的非监督学习过程。最初的关于学习非线性流形的机器学习研究集中于基于最近邻图形的非参数方法。此图中每个训练样本对应一个节点和边连接最近邻的节点。这些方法(Schölkopf et al., 1998; Roweis and Saul, 2000; Tenenbaum et al., 2000; Brand, 2003; Belkin and Niyogi, 2003; Donoho and Grimes, 2003; Weinberger and Saul, 2004; Hinton and Roweis, 2003; van der Maaten and Hinton, 2008)通过切平面联系这些节点，这些切平面通过变量和样本和其邻居的差向量的方向确定，如图14.8所示。
全局坐标系然后可以通过优化或者求解线性系统获得。图14.9展示了流形如何可以由大量局部线性高斯样斑块来平铺（或“煎饼”，因为高斯处于切线方向平面）。
然而这些流形学习的局部非参数方法有一个基本的困难，由Bengio和Monperrus (2005)提出：如果流形不是非常光滑（它们有许多波峰和波谷和曲折），可能需要非常大量的训练样本来还原这些每一个变量，根本没有机会范化到不可见变量。实际上，这些方法仅能通过临近样本间的插值来泛化流形的形状。不幸的是，人工智能问题涉及的流形有非常复杂的结构，通过局部插值是难以捕获的。考虑图14.6所示的图像转录对应的流形。如果我们仅仅观察输入向量xi的一个坐标，随着图像被转录，我们可以观察到这个坐标每次经过图像亮度的峰值或者谷值时都可以遇到一个波峰或者波谷。换句话说，亮度模式的复杂程度是驱动，由执行图像转录生成流形的复杂程度的图像模板。这促使了采用分布式表征和深度学习来捕获流形的结构。###14.7	收缩自编码（Contractive Autoencoders）收缩自编码(Rifai et al., 2011a,b)对编码器h = f (x)引入了一个显式的正则项，使得f的偏导数尽可能地小。
惩罚项Ω(h)是编码函数偏导数的雅可比矩阵的Frobenius范数平方。
去噪自动编码器和收缩自动编码器之间存在着如下联系：Alain and Bengio (2013)表明在有限的微高斯噪声输入下，去噪重构误差等同于收缩自编码中重建函数从x到r = g(f(x))映射的惩罚项。也就是说，去噪自动编码器使重建函数能够应对输入中尺寸有限的小扰动，而收缩自动编码器使得特征提取函数能够应对输入的无穷小扰动。在使用分类器时，如果应用基于雅克比的收缩惩罚进行特征的预训练，通常对f(x)而不是g(f (x))应用收缩惩罚能够获得最好的分类精度。对f(x)使用收缩惩罚也与得分匹配有着密切联系，如14.5.1所示。
收缩的名称源于CAE翘曲空间的方式。具体地说，因为CAE被训练用于应对其输入的扰动，它鼓励内的输入点映射到更小邻域内的输出点。我们可以认为这是收缩输入邻域到更小的输出邻域。这里需要澄清，CAE的收缩仅是局部的，一个训练样本的所有扰动都被映射到f(x)的附近。全局上，两个不同的点x和x1也许被映射到比原始两点间距离更远的f(x)和f(x1)点。映射f使这两者扩大或者远离数据流形是合理的（如图14.7中1维的简单例子所示）。当对sigmoid单元使用Ω(h)惩罚项时，一种收缩雅克比（矩阵）的方法是使sigmoid单元接近0或1。这鼓励CAE用sigmoid（函数）的极值来编码输入点，可以理解为二进制编码。这也保证了CAE展开其编码值时能够覆盖大部分sigmoid隐单元所包含的超立方体。
我们可以将点x的雅克比矩阵J理解为非线性编码器f(x)的一个线性算子近似。这允许我们更正式地使用“收缩”（contractive）一词。在线性算子理论中，如果Jx的范数对于所有单元范数x不大于1，那么一个线性算子可以称为是收缩的。换句话说，如果J缩小了单元球面那么J就是收缩的。我们可以认为CAE在每个训练样本点惩罚Xf(x)的局部线性近似的Frobenius范数，从而鼓励这些局部线性算子变得收缩。
如14.6所描述，正则化的自动编码器通过平衡两股相对的力量来学习流形。在CAE的情况下，这两种力量是重建误差和收缩惩罚项Ω(H)。单独的重建误差鼓励CAE学习到一个恒等方程。单独的收缩惩罚项鼓励CAE学习到关于X的常量特征。这两股力量之间的妥协产生了一个多数导数都非常小的自动编码器。只有对应于输入小部分方向的少数隐单元才有比较显著的导数。
CAE的目标是学习数据的流形结构。较大Jx对应的x方向会剧烈改变h，所以这些方向最有可能是近似流形正切面的方向。Rifai et al. (2011a)和Rifai et al. (2011b)的实验结果表明CAE的训练结果使得J的奇异值的幅值小于1，所以产生了压缩。然而，一些奇异值仍高于1，因为重建误差惩罚项鼓励CAE编码的方向具有最低的局部方差。最大的奇异值对应的方向可以被理解为CAE学习到的正切方向。理想情况下，这些正切方向对应于数据的真正内在变化。例如，CAE在应用到图像时应该学习到这样的切向量，即能够显示出图像中对象逐渐改变姿势时图像是如何改变的，例如图14.6所示。实验获得的奇异值向量的可视化似乎对应于输入图象中有意义的变换，如图14.10 所示。 
图14.10：
CAE正规化准则的一个实际问题是，虽然在单隐层自编码器的情况下很容易计算，但是在深层自动编码器的情况下计算代价要昂贵得多。Rifai et al. (2011a)等人采取的策略是分开训练一系列的单隐层自动编码器，其中每个自动编码器都被训练用于重建前一个自动编码器的隐含层。然后这些自动编码器就可以组成一个深度自动编码器。因为每一层都分别训练成局部收缩的，所以深度自动编码器也是收缩的。这样的结果虽然和联合整个架构加上一个雅克比惩罚项作为一个深度模型训练而得到的结果不同，但是仍然可以捕获希望得到的量化特性。另外一个实际问题是，如果我们不对解码器采用某种形式的缩放，收缩惩罚项可能获得无用的结果。例如，编码器可能由一系列小常数相乘的输入构成，那么解码器可能由对编码除以e构成。当e接近0时，编码器使得收缩惩罚项Ω(h)，从而不能学习到（输入的）任何分布。同时，解码器保持完全重构。在Rifai et al. (2011a)的工作中, 通过绑定权重系数f和g来防止这样的情况发生。f和g都是由仿射变换加逐元素非线性构成的神经网络标准层，因此它是可以直接设置g权重矩阵为f的权重矩阵的转置。###14.8	可预测稀疏分解（Predictive Sparse Decomposition）可预测稀疏分解（Predictive sparse decomposition, PSD）是由稀疏编码和参数化自动编码器混合构成的模型（Kavukcuoglu et al., 2008）。参数化自动编码器被训练用于预测迭代推理的输出。PSD已经被应用于图像和视频中物体识别的无监督特征学习(Kavukcuoglu et al., 2009, 2010; Jarrett et al., 2009; Farabet et al., 2011)，也被应用于音频领域(Henaff et al., 2011)。该模型包括一个参数化的编码器f(x)和一个参数化的解码器g(h)。在训练阶段，h由优化算法控制。训练通过最小化如下目标函数进行，
||x − g(h)||2 + λ|h|1 + γ||h − f(x)||2 	（14.9）
与稀疏编码类似，训练算法在关于h最小化和关于模型参数最小化间交替进行。关于h最小化非常迅速，因为f(x)为h提供了很好的初始值，而且损失函数限制了h无论如何都在f(x)附近。简单的梯度下降就能在短短十步之内获得h的合理值。
PSD使用的训练过程，与首先训练一个稀疏编码模型然后再训练f(x)去预测稀疏编码特征值的过程是不一样的。PSD的训练过程是正则化解码器，使其能够采用让f(x)推测出好的编码值的参数。
可预测稀疏编码是learned approximate inference的例子。该主题在章节19.5将有更多讨论。第19章介绍的工具清楚地表明， PSD可以被解释为通过最大化模型对数似然的下界来训练一个有向稀疏编码模型。
在PSD的实际应用中，迭代优化仅在训练过程中使用。当部署模型时，参数编码器f被用于计算学习到的特征。相比于通过梯度下降来推断h，评估f的计算成本更低。因为f是一个可微的参数函数，PSD模型可以堆叠并用于初始化一个深度网络，进而使用不同的标准来训练。###14.9	自编码的应用（Applications of Autoencoders）自编码已成功用于降维和信息检索任务。降维是表征学习和深度学习的最初应用程序之一。它也是研究自编码的早期动机之一。例如，Hinton and Salakhutdinov (2006) 训练堆叠的RBM，然后用他们的权重来初始化一个深层的隐层逐渐变小的自编码，最终的瓶颈在30个隐单元。编码产生的重建误差要低于用PCA使其降至30维，同时学习得到的表达更容易定性解释，并涉及到底层的类别，其中这些类别表现为分离良好的类簇。
低维表示可以提高许多任务的性能，例如分类。小空间模型的内存消耗和运行时间更少。许多形式的降维使得语义关联的样本空间相近，例如Salakhutdinov和Hinton (2007b)，以及Torralba et al. (2008)。映射到低维空间提供了范化的提示。
通常比从降维获得更多益处的一个任务是信息检索，在数据库中查找类似于查询条目的任务。该任务在同其他任务一样地从降维中获得益处之外，同时获得了额外的益处，因为搜索在特定的低维空间中会得到极端的效率提升。具体地，如果我们训练降维算法输出的编码是低维和二进制的，那么我们就可以在哈希表中映射二进制编码向量到条目，来存储数据库的全部条目。这个哈希表允许我们通过返回与查询具有相同的二进制编码的所有数据库条目，来进行信息检索。我们也可以非常有效地搜索稍微类似的条目，仅通过翻转查询编码的几个比特位。通过降低维数和二值化的方法进行信息检索被称为语义哈希（semantic hashing）(Salakhutdinov and Hinton, 2007b, 2009b)，已经被用于文本(Salakhutdinov and Hinton, 2007b, 2009b)和图像(Torralba et al., 2008; Weiss et al., 2008; Krizhevsky and Hinton, 2011)。
为输出语义哈希的二进制编码，人们通常在最终层上使用S形（sigmoid）的编码函数。Sigmoid单元必须被训练成对所有的输入都饱和到接近0或者1。一个可以完成这样任务的窍门是训练阶段在Sigmoid非线性前简单地注入噪声。噪声的幅度应该随时间增加。要战胜这种噪声和保存尽可能多的信息，网络必须增大S形函数的输入幅度，直至出现饱和。在一些研究方向中，学习散列哈希的思想已经被进一步探讨，包括如下想法，即训练特定的表征来优化损失，以便更直接地解决在哈希表中找到附近样本的任务(Norouzi and Fleet, 2011)。