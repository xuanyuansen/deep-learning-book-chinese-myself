##第十五章：表征学习（Representation Learning）在本章中，我们首先讨论表征学习的含义以及表征的概念如何对构建深层结构起到帮助。我们讨论学习算法如何在不同的任务间共享统计学的力量，包括使用从无监督任务中获得的信息来执行监督任务。共享表征对于处理多模式或领域的问题也是有帮助的，或者是迁移已学习知识到样本很少或者没有但却存在表征任务的任务。最后，我们回顾并讨论了表征学习成功的原因，首先是分布表征的理论优势，其次是深度表征(Hinton et al., 1986)，最后是更一般的有关数据生成的基本假设，特别是观测到数据的底层原因。
许多信息处理任务处理起来的难易程度取决于该信息是如何表示的。这个通用的原则同样适用于日常生活，一般的计算机科学，以及机器学习。例如，一个人会直接使用长除法来将210除以6。如果这个任务用罗马数字表示就没那么直观。如果问现代人将CCX除以VI，那么首先会将其转换为阿拉伯数字来表示，然后使用数值体系来完成除法。更具体地，我们可以量化使用不同表示进行运算的符号运行时间。例如，将数字插入有序数组的正确位置，如果该数组以链表表示那么时间复杂度是O(n)，如果用红黑树表示那么时间复杂度就是O(log n)。
在机器学习背景下，是什么使得一种表征优于其他？一般而言，好的表征使得后续的学习任务得以简化。而表征方法的选择通常取决于后续的学习任务。
我们可以把用监督学习训练的前馈神经网络理解为某种形式的表征学习。特别地，神经网络的最后一层通常是一个线性分类器，例如softmax回归分类器。神经网络的余下部分学习如何为这个分类器提供一个表征。使用监督准则进行训练通常会使得每一个隐层的表征都会保留使得分类任务容易的性质。例如，输入中线性不可分的类别会在最后一个隐层变得线性可分。原则上，最后一层可以变成另外一个模型，例如最近邻分类器（Salakhutdinov and Hinton, 2007a）。不同类型的最后一层也会使得倒数第二次的特征学习到不同的性质。
前馈神经网络的监督训练并不涉及对中间特征显式地设置任何条件。其他种类的表征学习算法通常显式地设计通过特殊的方式来shape表征。例如，假设我们想要学习一个表征来使得密度估计更容易。分布越具有独立性就越容易建模，所以我们想要设计一个目标函数，鼓励表征向量h的每个元素间都互相独立。与有监督神经网络类似，非监督深度学习算法有一个主要的训练目标，同时副作用地学习到了一个表征。忽略表征是如何学习得到的，表征可以用于另一个任务。（545页，it can can be错误）另外，多任务通过共享中间表征可以共同学习（一些有监督，一些非监督）。
大多数表征学习任务面临保留输入的更多信息和获得漂亮性质（例如独立性）间的权衡。
表征学习特别有趣因为其提供了一种进行无监督和半监督学习的方式。我们通常有大量的未标记训练数据和相对很少的有标签数据。在有标签的训练集上使用有监督的机器学习技术来训练通常导致严重的过拟合。半监督学习通过从未标记数据中学习，从而提供了解决这样问题的机会。特别地，我们可以从未标记数据中获得好的表征，然后使用这些表征来解决监督学习任务。人和动物能够从很少的标记样本中学习。然而我们并不知道为什么这是可能的。许多因素可以解释人类的高效表现，例如大脑可能使用大量的分类器或者贝叶斯推理技术进行集成。一个流行的假说是大脑能够利用无监督或者半监督学习。有许多可以利用无标签数据的方法。在本章中，我们专注于可以通过未标记数据来学习较好表征的假设。###15.1	贪心逐层无监督预训练（Greedy Layer-Wise Unsupervised Pre-training）无监督学习对深度神经网络的复兴起到了关键的历史作用，首次允许在没有特殊网络结构的情况下训练深度神经网络，例如卷积或者循环结构。我们称此过程为无监督训练，或更精确地说，贪心逐层无监督预训练。这是过程是从一个任务中学习到的表征（无监督学习，试图捕捉输入的分布信息）有时会对其他任务起作用的一个典型例子（具有相似输入域的监督学习）。
贪心逐层无监督预训练依赖于单层表征学习算法，例如RBM，单层的自编码器，稀疏编码模型，或者其它能够学习隐式表征的模型。每一层都通过无监督学习来预训练，采用前一层的输出作为输入，同时产生的输出作为数据新的表示，其分布（或其与其它变量的关系，例如待预测的标签）很有希望变得简单。见15.1算法的正式描述。
基于无监督训练标准的贪心逐层训练早已被用来回避联合训练有监督任务中神经网络的所有隐层的困难。这种方法至少可以追溯到Neocognitron (Fukushima, 1975)。2006年深度学习的复习开始于这样的发现，即这样的贪心训练过程可以被用于为联合训练所有隐层找到一个非常好的初始化，而且这样的方法甚至可以成功训练全连接的网络结构(Hinton et al., 2006; Hinton and Salakhutdinov, 2006; Hinton, 2006; Bengio et al., 2007; Ranzato et al., 2007a) 。在这个发现之前，只有卷积深网络或者深度循环网络被认为是可行的训练。今天，我们现在知道，训练全连接深的架构不一定需要贪心逐层预训练，但无监督预训练的做法是第一个成功的方法。
贪心逐层预训练之所以被称为贪心的，是因为其是一个贪心算法，这意味着它每次独立优化解决方案的一个部分，而不是联合优化所有的部分。其被称为逐层是因为这些独立的部分是网络的每一层。特别地，贪心逐层预训练每一次只产生一个隐层，在训练第k层时保持前置层恒定。特别地，底层（首先训练的）在上层引入后是不变的。其被称为非监督的是因为每一个隐层都通过非监督表征算法来训练。然而其也被称为预训练，因为其被当做将联合训练算法用于优化调整所有隐层前的第一步。在一个监督学习任务的情况下，它可以被看作正则化（在某些实验中，预训练可以降低测试误差而不是降低训练误差）和参数初始化的一种形式。
通常使用“预训练”这个词不仅指预训练本身，而是结合了预训练阶段和监督训练阶段这两个完整的标准流程。监督学习阶段涉及在预训练阶段学习到的特征之上训练一个简单的分类器，或者涉及在预训练阶段优化调整整个神经网络。无论什么样的无监督学习算法或模型，在绝大多数情况下，整体的训练计划几乎是相同的。然而无监督学习算法的选择将明显影响其中的细节，无监督预训练的大多数应用遵循这一基本协议。
贪心逐层预训练也可以被用作初始化其它无监督学习算法，例如深自编码器(Hinton and Salakhutdinov, 2006)和包含需要隐变量层的概率模型。这些模型包括深度置信网(Hinton et al., 2006)，深度玻尔兹曼机(Salakhutdinov and Hinton, 2009a)。这些深度生成模型将在第20章加以说明。
如章节8.7.4所示，也可以有贪心逐层监督预训练。这是建立在训练浅层网络比训练深度网络容易的前提下，而这似乎已经在很多情况下得到验证(Erhan et al., 2010)。####15.1.1	何时以及为什么无监督预训练是有效的？在许多任务中，贪心逐层无监督预训练在分类任务中，能够对测试误差带来实质性的改善。该观点推动了2006年深度神经网络开始重新获得注意(Hinton et al 2006; Bengio et al., 2007; Ranzato et al., 2007a) 。然而在许多其他任务上，无监督预训练则不会带来好处，甚至会产生明显的伤害。Ma et al. (2015) 研究了预训练对用于预测化学反应的机器学习模型的影响，平均来说，预训练略为有害，但是对于许多任务却是显著有益的。因为无监督预训练有时是有益的，但是更多时候是有害的，所以理解何时以及为什么其有效的就非常重要，这样才可以决定其是否应该用于某特定任务。
首先必须明确指出，大多数这种讨论仅限于贪心无监督预训练。有其他完全不同的利用神经网络进行半监督学习的模式，例如章节7.13描述的虚拟对抗训练（virtual adversarial training）。另外，也可以在监督训练的同时训练一个自编码器或者生成模型。这种单级方法的例子包括判别RBM（discriminative RBM (Larochelle and Bengio, 2008) ），和梯形网络（ladder network (Rasmus et al., 2015) ），其中总的目标是两项的显式求和（一项使用标签，另一项使用输入）。
无监督预训练结合了两种不同的观点。首先，它利用了深度神经网络初始参数的选择可以对模型起到显著的正则化的效果的想法（以及，在较小程度上，它可以改善优化）。其次，它利用了学习输入的分布有助于学习从输入到输出的映射这样更普遍的想法。
这两种观点涉及机器学习算法几部分之间尚未被完全理解的许多复杂相互作用。
第一个想法，即初始参数的选择对于深度神经网络可能会对它的表达能力起到强烈的正则化效果，这是最容易理解的。在预训练开始流行的时候，其被理解为初始化模型到一个使其接近局部最小值的位置的方法。现在，局部最小值不再被认为是优化神经网络的一个严重问题。现在我们知道，我们的标准神经网络训练过程通常不会到达任何形式的临界点。预训练仍然可能使模型初始化在一个不可访问的位置，例如被目标函数在不同mini-batch上变化非常剧烈的区域包围的区域，这样会对梯度的估计带来很多噪声，或者被这样的区域包围，其Hessian矩阵非常糟糕以至于只能通过很小的步长来完成梯度下降。但是，我们明确指出预训练中那些方面在监督训练中被保留的能力仍然是有限的。这是一个原因，现代方法通常同时使用无监督学习和监督学习，而不是在两个连续的阶段中使用。监督学习阶段的优化到底如何保留无监督学习阶段中的信息？人们也可避免挣扎于这些复杂的想法，即简单地冻结特征提取的参数，只使用监督学习在学习到的特征之上增加一个分类器。
另外一个想法比较好理解，学习算法可以使用在无监督学习阶段获得的信息来更好地执行监督学习阶段。基本思想是一些在无监督任务中有用的特征在监督学习任务仍然有用。例如，如果我们训练汽车和摩托车的的图像生成模型，它将需要了解轮子，以及图像中应有多少轮子。如果我们很幸运，车轮的表征形式对于有监督学习是很容易使用的。然而这在数学和理论层面尚不清楚，因此哪些任务将从无监督学习中以这种方式获益并不总是可预测的。这种方法的许多方面是高度依赖于所使用的特定模型。例如，如果我们想在预训练特征上添加一个线性分类器，特征必须使底层类别线性可分。这些特性往往自然发生，但并不总是这样。这是另一个原因，有监督和无监督学习都是可取的，由该输出层施加的约束从一开始就自然地包括在内。
从把非监督预训练当做表征学习的角度看来，我们期待非监督预训练在初始表征较差的情况下更有效。这种使用方式的一个重要例子就是词嵌入（word embedding）。通过一位有效编码向量来表示词时其信息不够丰富，这是因为任意两个不同的一位有效编码向量之间的距离是一样的（2的L2平方距离）。学习到的词嵌入自然地通过词汇互相之间的距离编码了他们之间的相似度。因此，无监督预训练在处理词时是特别有用的。当处理图像时其作用略差，这可能是因为图像已经表示在一个丰富的向量空间，其中距离是一个质量较差的相似性度量。
从将非监督预训练当做正则化的角度看来，我们期望在有标签样本的数量非常少的情况下非监督预训练作用最大。因为非监督预训练的信息来源是无标签样本，我们也预期在非监督预训练在无标签样本的量很大时性能最佳。通过大量无标签样本和少量有标签样本的非监督预训练来进行半监督学习的优势在2011年有特别的体现，期间非监督预训练赢得了两项国际迁移学习比赛（Mesnil et al., 2011; Goodfellow et al., 2011），其设定为任务中的有标签样本非常少（from a handful to dozens of examples per class）。这些影响也在Paine et al. (2014) 精心控制的实验中有记录。
其他因素可能也会涉及到。例如，当待学习的目标函数极端复杂时无监督预训练是最有用的。无监督学习与正则化不同（例如权重衰减），因为其使得学习算法发现对于非监督学习任务有益的表征函数，而不是发现一个简单的映射函数。如果真实的底层函数非常复杂且其形状受到输入分布的限制，那么无监督学习将会是更适合的正则项。
除这些注意事项以外，我们现在分析一些成功案例，即明确得知其中无监督预训练导致了改善，并解释我们已知的为什么发生这种改进。无监督预训练通常用于改进分类器，并且从减少测试集误差的观点来看通常是最有趣的。然而，无监督预训练可以帮助除分类之外的任务，并且可以用于改进优化而不仅仅是作为正规项。 例如，它可以改善深度自动编码器的训练和测试重建误差（Hinton和Salakhutdinov，2006）。
Erhan et al. (2010) 进行了许多实验来解释无监督预训练的几个成功例子。对训练误差的改进和对测试误差的改进可以用无监督预训练的方式来解释，即无监督预训练将参数引入到否则将不可访问的区域中。神经网络训练是非确定性的，并且在每次运行时收敛到不同的函数。训练可以在梯度变小的点停止，早期停止来结束训练以防止过度拟合的点，或者在梯度大但是由于随机性或者Hessian矩阵糟糕的情况下难以找到下降的一步。接受无监督预训练的神经网络在函数空间的相同区域中一直停止，而没有预训练的神经网络在另一个区域中一直停止。参加图（15.1）对这种现象的可视化。预训练网络到达的区域较小，这表明预训练减少了估计过程的方差，这又可以降低严重过拟合的风险。换句话说，无监督预训练将神经网络参数初始化到它们不可逃逸的区域，并且该初始化之后的结果更加一致，同时更不可能比没有该初始化时糟糕。Erhan et al（2010）也提供了一些关于何时预训练工作最好的答案，当测试误差的平均值和方差被深层网络预训练最大程度地减少。 请记住，这些实验是用于训练深层网络的现代技术发明和普及之前进行的（例如Re-LU，dropout和batch normalization），因此无监督预训练与当代方法结合的效果是较少知道的。
一个重要的问题是如何无监督预训练为何可以起到正则化的作用。一个假设是，预训练鼓励学习算法发现与生成观察数据的根本原因相关的特征。除了无监督预训练之外，这也是激励许多其它算法的重要思想，并且在第15.3章中进一步描述。
使用无监督学习与结合这种信念的其它方法相比，无监督预训练具有缺点，即它使用两个单独的训练阶段来操作。这两个训练阶段有缺点的一个原因是，没有单个超参数可预测地减少或增加由无监督预训练产生的正则化的强度。相反，存在非常多的超参数，其效果可以在事实之后测量，但是通常难以提前预测。当我们同时执行无监督学习和监督学习而不是使用预训练策略时，存在单个超参数，通常是附加到无监督损失函数的系数，由其来确定无监督目标正则化监督模型的强度。通过减小该系数，总是能够可预测地获得较少的正则化效果。在无监督预训练的情况下，没有灵活适应正则化强度的方式——或者监督模型被初始化为预训练的参数，或者不是。
有两个单独的训练阶段的另一个缺点是，每个阶段具有其自己的超参数。 第二阶段的性能通常不能在第一阶段期间预测，因此第一阶段的超参数可以使用来自第二阶段的反馈来更新它们之间存在长的延迟。 最标准的方法是在监督阶段中使用验证集误差，以便选择预训练阶段的超参数，如Larochelle等人 （2009）的讨论。 在实践中，对无监督目标使用早期停止，一些超参数在预训练阶段期间可以方便地设置，如预训练迭代的次数，然而这并不是理想的，虽然在计算上比使用监督目标代价小得多。
今天，除了在自然语言处理领域，其中one-hot编码的自然表示不能传达相似性信息，且存在非常大的未标记的集合可用的情况下，无监督的预训练已经大部分被放弃。在这种情况下，预训练的优点是可以对一个巨大的未标记集合（例如使用包含数十亿字的语料库）进行预训练，学习良好的表示（通常是单词，但也是句子），然后使用这个表示或其微调后的表示，用于训练所包含较少示例的监督任务。 这种方法的先驱是Collobert and Weston (2008b), Turian et al. (2010), and Collobert et al. (2011a)，今天仍然普遍使用。
基于监督学习的深度学习技术，通过Dropout或者批规范化，能够在很多任务上实现人类级别的性能，但只能使用极大的标记数据集。这些相同的技术胜过在中等大小的数据集上的无监督预训练，例如CIFAR-10和MNIST，其中每个类大约有5,000个标记的示例。在极小的数据集上，如可选择拼接数据集，贝叶斯方法优于基于无监督预训练的方法（Srivastava，2013）。由于这些原因，无监督预训练的流行程度已经减少。然而，无监督预训练仍然是深度学习研究历史上的一个重要里程碑，并继续影响当代方法。预训练的概念作为一种非常常见的转移学习方法，已经推广到章节8.7.4中讨论的监督预训练。对于在ImageNet数据集预训练的卷积网络，用监督预训练进行转移学习是流行的（Oquab et al., 2014; Yosinski et al., 2014）。为了这个目的，实践者公布这些网络训练的参数，就像为自然语言任务公布预训练的单词向量一样（Collobert et al., 2011a; Mikolov et al., 2013a）。
###15.2	转移学习和领域适应(Transfer Learning and Domain Adaptation)转移学习和领域适应指的是利用在一个设置（即分布P1）中所学习的内容来改善另一个设置（例如分布P2）下的泛化情况。 这推广了上一节中提出的想法，其中我们在无监督学习任务和监督学习任务之间转移表征。
在转移学习中，学习者必须执行两个或更多不同的任务，但是我们假设解释P1变化的许多因素与需要为学习P2来捕获的变化相关。这通常可以在监督学习上下文中理解，其中输入是相同的，但是目标可以具有不同的性质。例如，我们可以在第一个设置中学习一组视觉类别，例如猫和狗，然后在第二个设置中学习不同的视觉类别集合，例如蚂蚁和黄蜂。如果在第一设置（从P1采样）中存在更多的数据，则这可以帮助从P2中很少的样本学习到快速泛化的有用表示。许多视觉类别共享边缘和视觉形状的低级概念，几何变化的影响，光照变化等等。一般来说，转移学习，多任务学习（章节7.7）和域适应，当存在对于不同设置或任务有用的特征时，即对应于出现在多于一个设置中的潜在因素，可以通过表征学习来实现。这在图7.2中被证实，具有共享的下层和任务相关的上层。
然而，有时候，不同任务之间共享的不是输入的语义，而是输出的语义。例如，语音识别系统需要在输出层产生有效的句子，但是输入附近的较早层可能需要识别相同音素或子音素发音却非常不同的版本，这取决于哪个人正在说话。 在这样的情况下，更有意义的是共享神经网络的上层（输出附近），并且具有针对任务特定的预处理，如图15.2所示。
在领域适应的相关情况下，任务（和输入到输出的最优映射）在每个设置之间保持相同，但是输入的分布则稍有不同。例如，考虑情感分析的任务，其包括确定评论是否表达积极或消极情绪。发表在网上的评论来自许多类别。当在诸如书，视频和音乐的媒体内容的顾客评论上训练的情绪预测器，随后被用于分析关于诸如电视机或智能电话的消费电子产品的评论时，领域适应情景可能出现。可以想象，有一个深层函数可以告诉任何语句是正面的，中性的还是负面的，但是词汇和风格当然可能会因域而异，使得跨领域的泛化变得更加困难。已经发现简单的无监督预训练（使用去噪自动编码器）对于使用领域适应的情感分析非常成功（Glorot et al., 2011b）。
一个相关的问题是概念漂移，我们可以将其视为转移学习的一种形式，因为数据分布随时间逐渐变化。 概念漂移和转移学习都可以被视为多任务学习的特定形式。尽管短语“多任务学习”通常指的是监督学习任务，但是更一般的转移学习的概念也适用于无监督学习和强化学习。
在所有这些情况下，目标是利用来自第一个设置的数据来提取，在学习甚至直接在第二设置中进行预测时可能有用的信息。 表征学习的核心思想是相同的表征在两种设置下都是有用的。 在两个设置中使用相同的表征允许表征从可用于两个任务的训练数据中都受益。
如前所述，使用无监督深度学习的转移学习已经在一些机器学习竞赛中取得成功（Mesnil et al., 2011; Goodfellow et al., 2011）。在第一个比赛中，实验设置如下。首先给每个参与者来自第一设置（来自分布P1）的数据集，证明一些类别的示例。参与者必须使用它们来学习一个良好的特征空间（将原始输入映射到某个表示），这样当我们将这个学习到的变换应用于来自转移设置（分布P2）的输入时，可以用很少的标记样本来训练一个线性分类器。在这个竞争中发现的最引人注目的结果之一是，如果一个架构使用越深层的表示（从纯粹无监督的方式学习在第一个设置P1收集的数据），那么其在第二个设置P2下的关于新类别的学习曲线（转移）会变得更好。对于深度表征，只需要较少的转移（学习）任务的标记样本就能实现明显的渐近泛化性能。
转移学习的两种极端形式是一次性学习和零点学习，有时也称为零数据学习。 对于一次性学习，仅给出转移任务的一个标记示例，而对于零次学习任务则没有给出任何标记示例。
一次性学习（Fei-Fei et al., 2006）是可能的，这是因为在第一阶段学习到的表征可以清楚地分开基础类别。 在转移学习阶段期间，仅需要一个标记的示例来推断，在表征空间中的相同点周围聚集的许多可能的测试示例的标签。 这种工作方式使得对应于这些不变性的变化因子，在学习到的表征空间中与其他因素完全分离，并且我们以某种方式学习到了，当区分某些类别的对象时哪些因素是有意义的，还是无关紧要的。
至于零点学习的示例，考虑让学习者读取大量文本然后解决对象识别的问题。 如果文本可以足够好地描述对象，则可以识别特定对象类，即使没有看到该对象的图像。 例如，学习者读过猫有四条腿和尖尖的耳朵，即使以前没有看过猫，也可能能够猜测是否是一只猫图像。
零数据学习（Larochelle et al., 2008）和零点学习（Palatucci et al., 2009; Socher et al., 2013b）只有在训练期间利用了额外的信息才是可能的。我们可以认为零数据学习场景包括三个随机变量：传统输入x，传统输出或目标y，以及描述任务T的附加随机变量。训练模型来估计条件分布p（y | x，T），其中T是对我们希望模型来执行的任务的描述。在我们的例子中，在已经阅读了猫之后识别猫，输出是二进制变量y，其中y = 1表示是，y = 0表示否。任务变量T表示要回答的问题，例如，在这个图像中是否有一只猫？如果我们有一个训练集，其包含与T存在于同一空间中对象的无监督样本，我们可能能够推断未知的T实例的含义。在我们的例子中，在没有看到猫的图像的情况下识别猫，重要的是我们有未标记的文本数据，包含例如“猫有四条腿”或“猫有尖的耳朵”这样信息的句子。
零点学习要求T允许以某种泛化的方式表示。 例如，T不能仅仅是指示对象类别的one-hot编码。 Socher et al. (2013b) 通过使用与每个类别相关联的词而学习到的词嵌入，来代替地提供对象类别的分布式表示。
类似的现象也发生在机器翻译中（Klementiev et al., 2012; Mikolov et al., 2013b; Gouws et al., 2014）：我们有一种语言的单词，单词之间的关系可以从非语言学语料库学习; 另一方面，我们已经翻译了句子，其中一种语言的单词到另一种语言中相关的单词。 即使我们可能没有标记的例子将语言X中的字A翻译成语言Y中的字B，我们可以推广和猜测字A的翻译，因为我们已经学习了语言X中的单词的分布式表示， 语言Y中的单词的分布式表示，并且经由由两种语言的匹配的句子组成的训练示例，创建了两个空间相关的链接（可能是双向的）。 如果所有三种成分（两种表示形式和它们之间的关系）被共同学习，这种转移(学习)将是最成功的。
零点学习是一种特殊形式的转移学习。同样的原则解释了如何执行多模式学习，捕获一种模态中的表示，另一种（模态）中的表示，以及在一个模态中的观察x和在另外一个模态中的观察y组成的（x，y）之间的关系对（一般为联合分布）（Srivastava和Salakhutdinov，2012）。 通过学习所有三组参数（从x到它的表示，从y到它的表示，以及两个表示之间的关系），一个表示中的概念被锚定在另一个表示中，反之亦然，允许有意义地推广到新的关系对。步骤如图15.3所示。Figure 15.3: ###15.3（半监督解释因素）Semi-Supervised Disentangling of Causal Factors 关于表征学习的一个重要问题是“什么使得一个表征比另一个更好？”一个假设是，理想表征是该表征中的特征对应于所观察数据的底层原因，在特征空间中不同原因对应单独的特征或者方向，使得表征能够解释彼此不同原因。 这个假设激励了我们首先寻找p（x）良好表征的方法。 如果y是x的最显著原因之一，则这种表征也可以用做计算p（y | x）的良好表征。至少从20世纪90年代以来，这个观点就引导了大量关于深度学习的研究（Becker and Hinton, 1992; Hinton and Sejnowski, 1999）。关于半监督学习可以胜过纯监督学习的其他论点，我们推荐读者参考章节1.2中的内容。
在表示学习的其他方法中，我们经常关注容易建模的表示，例如，其条目稀疏或彼此独立的表示。清晰地分离潜在因果因素的表征可能不一定是容易建模的。然而，这个假设的另一部分是，对于许多AI任务，可以通过无监督的表征学习来激励半监督学习，这两个属性是一致的：一旦我们能够获得对我们观察的基本解释，则从其他属性分离个体属性通常变得容易。 具体来说，如果表征h表示观察到的x的许多根本原因，并且输出y是最突出的原因之一，则容易从h预测y。首先我们看到，因为p(x)的无监督学习不能帮助学习p(y|x)，半监督学习会失效。例如，考虑p(x)是均匀分布，并且我们想要学习f(x)=E [y | X]。显然，仅观察x值的训练集并没有给我们关于p(y|x)的信息。
接下来，让我们看一个简单的半监督学习如何成功的例子。考虑x来自于混合（分布）的情况，每个y值具有一个混合分量，如图15.4所示。如果混合分量是良好分离的，则模型p(x)可以精确地揭示每个分量在哪里，并且每个类别单个标记的示例将足以完美地学习p(y|x)。但是更一般地，什么可以使p(y|x)和p(x)紧密联系在一起？
如果y与x具有密切相关的因果关系，则p(x)和p(y|x)将被严格地绑定，那么将无监督表征学习作为半监督学习策略，来尝试解开变化的潜在因素可能是有用的。
假设y是x的因变量之一，并且h表示所有这些因子。真实的生成过程可以被构想为，根据一个有向图形模型来构造，其中h作为x的父（节点）：
p(h, x) = p(x | h)p(h) 
因此，数据的边缘概率为，
p(x) = Ehp(x | h)
从这个直观的角度观察，我们可以得出结论，x的最好可能模型（广义上的观点）是能够揭示上述“真实”结构，且h作为隐变量能够解释观察到的x的变化。上述讨论的“理想的”表征学习理应能够恢复这些隐变量，如果y是这些隐变量中的一个（或与它们中的一个密切相关），则从这种表示来学习预测y将非常容易。我们还看到，给定x条件下的y的条件概率分布被贝叶斯规则绑定到上述等式中的分量，
p(y | x) = p(x | y) p(y)/ p(x)。
因此边缘概率p（x）与条件概率p（y | x）密切相关，有关前者结构的知识应该有助于学习后者。 因此，在这些假设的情况下，半监督学习应该能够提高性能。
一个重要的研究问题是，大多数观察是由极其大量的潜在原因形成的。 假设y = hi，但是无监督学习者不知道是哪个hi。 蛮力解决方案是一个无监督学习者学习一个表示，它捕获所有合理的显著生成因子hj并将它们彼此分开，从而使得从h容易预测y，而不管哪个hi与y相关联。在实践中，蛮力解决是不可行的，因为捕获影响观察变化的所有或大多数因素是不可能的。例如，在视觉场景中，表征应该始终编码背景中所有的最小对象吗？在一个记录良好的心理现象中，人类并没有察觉到他们所处环境的变化，这些变化与他们正在进行的任务并不直接相关——参见例如，Simons和Levin（1998）。半监督学习的一个重要研究前沿是确定在每种情况下要编码的内容。 目前，处理大量底层原因的两个主要策略是，在与无监督学习的同时使用监督学习，使得模型将选择捕获最相关的变化因素，或者使用更大的表示，如果使用纯粹的无监督学习。
无监督学习的一个新兴策略是修改哪些是最为突出的根本原因的定义。 历史上，自动编码器和生成模型被训练来针对固定标准进行优化，通常类似于均方误差。 这些固定标准确定了哪些原因被认为是显着的。 例如，应用于图像像素的均方误差隐含地指定，如果其显著地改变大量像素的亮度，则潜在原因是显著的。 如果我们希望解决的任务涉及与小对象的交互，这可能是有问题的。 参见图15.5，其展示了在机器人技术任务中自动编码器未能学习编码小乒乓球的示例。 这个同样的机器人能够成功地与更大的对象（例如棒球）交互，因为这些对象根据均方误差是更突出的。
显着性的其他定义也是可能的。例如，如果一组像素遵循高度可识别的图案，即使该图案不涉及极端的亮度或暗度，则该图案可以被认为是非常突出的。实现这样突出定义的一种方法是使用最近开发的被称为生成式对抗网络的方法（Goodfellow等人，2014c）。在这种方法中，训练生成模型以迷惑前馈分类器。前馈分类器尝试将来自生成模型的所有样本识别为假的，并将来自训练集合的所有样本识别为真实的。在这个框架中，前馈网络可以识别的任何结构化模式都是非常突出的。生成式对抗网络将在章节20.10.4中更详细地描述。至于当前讨论的目的，这足以理解他们学习如何确定什么是显着的。 Lotter等人（2015）表明，训练生成人类头部图像的模型在使用均方误差训练时，往往忽略生成耳朵，但在用对抗框架训练时将成功生成耳朵。因为耳朵与周围皮肤相比不是非常明亮或黑暗，所以根据均方误差损失它们不是特别突出的，但是它们高度可识别的形状和一致的位置，意味着前馈网络可以轻易地学习检测它们，使得它们在生成对抗框架下高度突出。参见图15.6中的示例图像。生成对抗网络只是确定哪些因素应该表征的一个步骤。我们期望未来的研究将发现更好的方法来确定表征哪些因素，并开发根据任务表征不同因素的机制。
学习基本因素的好处，正如Schölkopf（2012）等人指出的，是如果真正的生成过程有x作为效果，y作为一个原因，那么建模p（x | y）对p（y）的变化是鲁棒的。如果因果关系被逆转，这将是不真实的，因为通过贝叶斯规则，p（x | y）将对p（y）的变化敏感。通常，当我们考虑由于不同领域，时间非平稳性或任务性质的变化而导致的分布变化时，因果机制保持不变（“宇宙的定律是恒定的”），而边缘分布的根本原因却可以改变。 因此，通过学习试图恢复因果关系h和p（x | h）的生成模型，可以预期对所有种类的变化具有更好的泛化和鲁棒性。
###15.4	（分布式表征）Distributed Representation概念的分布式表示——表示由许多可以彼此分开的元素组成——是表征学习的最重要的工具之一。分布式表征是强大的，因为它们可以使用具有K个值的N个特征来描述KN个不同的概念。正如我们在本书中看到的，具有多个隐单元的神经网络和具有多个隐变量的概率模型都利用了分布式表征的策略。 我们现在再介绍一个观察。许多深度学习算法是由隐单元可以学习表示能够解释数据潜在因果因素的假设来驱动的， 如章节15.3中所述。 分布式表征自然是这种方法，因为表示空间中的每个方向可以对应于底层配置变量不同的值。
分布式表示的一个示例是包含n个二进制特征的向量，其可以采取2n个配置，每个配置可能对应于输入空间中的不同区域，如图15.7所示。这可以与符号表示相比较，其中输入与单个符号或类别相关联。如果字典中有n个符号，则可以想象存在n个特征检测器，每个对应于检测到相关类别的存在。在这种情况下，只有n个不同的表征空间配置是可能的，在输入空间中切开n个不同的区域，如图15.8所示。这样的符号表示也称为one-hot表示，因为它可以由相互排斥的n位二进制向量捕获（它们中只有一个是处于激活状态的）。符号表示是更一般的类别非分布式表征的特例，其可以包含许多条目，但是条目之间并没有显著意义的区分性。
基于非分布式表征的学习算法的示例包括：
*	•聚类方法，包括k-means算法：每个输入点只分配一个类簇。*	•k-最近邻算法：一个或几个模板或原型示例与给定输入相关联。在k> 1的情况下，有多个值描述每个输入，但是它们不能彼此分开控制，因此这不能作为真正的分布式表示。*	•决策树：在给定输入时，只有一个叶子节点（以及从根到叶的路径上的节点）被激活。*	•高斯混合（模型）和专家混合（模型）：模板（聚类中心）或专家现在与一定程度的激活相关联。与k-最近邻算法一样，每个输入用多个值表示，但是这些值不容易彼此分开地控制。*	•具有高斯内核（或其他类似的局部核函数）的核机器：尽管每个“支持向量”或示例模板的激活程度现在都是连续值，但是与高斯混合（模型）有相同的问题。*	•基于n-gram的语言模型或翻译模型。根据后缀的树结构来划分上下文集合（符号序列）。例如，叶子节点可以对应于w 1和w 2的最后两个字。为树的每个叶子节点单独估计参数（可能有一些共享）。
* 对于部分这些非分布式算法，部分输出是不恒定的，而是相邻区域之间的内插值。参数（或示例）的数量与它们可以定义的区域的数量之间的保持线性关系。
将分布式表示与符号性表示区分开的一个重要相关概念是，由不同概念之间共享属性而产生的归纳。作为纯符号，“猫”和“狗”彼此远离任何其他两个符号。然而，如果将它们与有意义的分布式表征相关联，那么关于猫的许多事情可以推广到狗，反之亦然。例如，我们的分布式表征可以包含诸如“has fur”或“number of legs”这样的条目，其具有与“猫”和“狗” 相同的嵌入值。对单词的分布式表征进行操作的神经语言模型比其他直接操作单词的one-hot表示的模型泛化能力好得多，如章节12.4中的讨论。分布式表征归纳了丰富的相似性空间，其中语义上接近的概念（或输入）在距离上接近，这是纯粹的符号表示中缺少的属性。
何时和为什么使用分布式表征作为学习算法的一部分可以有统计优势？当使用少量参数可以紧凑地表示明显复杂的结构时，分布式表征可以具有统计优点。一些传统的非分布式学习算法仅由平滑性假设而推广，其表示为如果u≈v，则要学习的目标函数f具有通常f（u）≈f（v）的属性。有很多方法来形式化这样一个假设，但最终的结果是，如果我们有一个例子（x，y），我们知道f（x）≈y，那么我们选择一个估计函数fˆ，当我们移动到附近的输入x + e时，以尽可能少的改变来近似满足这些约束。显然这个假设是非常有用的，但是它遭受维数灾难的影响：为了学习在许多不同区域中增加和减少许多次的目标函数，我们可能需要至少与可区分的区域数量一样大的多个示例。可以将这些区域中的每一个认为是类别或符号：通过对每个符号（或区域）具有单独的自由度，我们可以学习任意从符号到值的解码器映射。 然而，这并不允许我们推广新的符号到新的区域。
如果我们足够幸运，目标函数除了平滑之外可能有一些规律性。 例如，具有最大池化层的卷积网络可以识别对象，而不管其在图像中的位置，即使对象的空间平移可能不对应于输入空间中的平滑变换。
让我们研究一个分布表征学习算法的特殊情况，通过对线性函数的输入增加阈值处理来提取二进制特征。该表示中的每个二元特征将D维空间R分成一对的半空间，如图15.7所示。相应的半空间的交叉数量n的指数数量决定了该分布式表征学习器可以区分多少区域。 D维空间R中的n个超平面的排列将产生多少个区域？ 考虑到有关超平面交集的一般结果（Zaslavsky，1975），可以证明（Pascanu et al., 2014b）这个二元特征表示可以区分的区域的数量是，因此，我们看到输入大小呈指数增长和隐含单元的数量呈多项式增长。
这为解释分布式表征的泛化能力提供了几何论据：使用O（nd）参数（对于D维空间R中的n个线性阈值特征），我们可以清楚地表示输入空间中的O（nd）区域。如果我们没有对数据做任何假设，并且对每个区域用一个唯一符号的表示，并且每个符号使用单独的参数来识别其对应的D维空间R中的部分，则指定O（nd）区域将需要O（nd ）个例子。更一般地，有利于分布式表征的论证可以扩展到这样的情况，我们对分布式表征中的每个属性使用非线性的，可能连续的特征提取器来代替线性阈值单元。在这种情况下的参数是，如果有k个参数的参数变换可以来学习输入空间中的r个区域，其中k远小于r，并且如果获得这样的表示对于所感兴趣的任务是有用的，我们非常可能用这种方式来泛化，比在非分布式设置中获得更好的效果，其中我们将需要O（r）个示例来获得相同的特征，以及将关联的输入空间分区成r个区域。使用较少的参数来表示模型意味着我们需要拟合的参数较少，并且因此只需较少的训练样本就可以很好地泛化。
对于为什么基于分布式表征的模型泛化较好的论证的另一部分是，尽管能够对这么多不同区域进行明确编码，但它们的容量仍然有限。例如，具有线性阈值单位的神经网络的VC维仅为O（w log w），其中w是权重的数量（Sontag，1998）。出现这种限制是因为，虽然我们可以为表示空间分配很多唯一的代码，但我们不能绝对使用所有的代码空间，也不能使用线性分类器来学习从表示空间h到输出y的任意函数映射。使用分布式表征与线性分类器的组合则表示了这样的先验信念，即待识别的类别是线性可分的，作为潜在因果因素的捕获函数h。我们通常想要学习类别，例如所有绿色对象的所有图像的集合或汽车的所有图像的集合，而不是需要非线性的XOR逻辑的类别。例如，我们通常不想将所有红色汽车和绿色卡车的数据划分为一个类别，将所有绿色汽车和红色卡车的集合划分为另一类。
到目前为止讨论的想法仍是抽象的，但是它们可以通过实验来验证。 Zhou et al. (2015) 发现，在ImageNet和Places基准数据集上训练的深层卷积网络中的隐藏单元学习到的特征其通常是可解释的，对应于人们自然分配的标签。在实践中，隐藏单元肯定不能总是学习一种这样具有简单语言名称的东西，但有趣的是，这在最好的计算机深度视觉网络的顶层附近出现。这些功能的共同之处在于，我们可以想象学习每个功能的时候不必看到所有其他功能的所有配置。 Radford et al. (2015) 证明了生成模型可以学习面部的图像表示，在表示空间中的单独方向捕获不同的潜在变化因素。图15.9示出了表示空间中的一个方向对应于该人是男性还是女性，而另一个方向对应于该人是否戴着眼镜。这些功能是自动发现的，而不是先验固定的。没有必要为隐藏单位分类器设置标签：对感兴趣的目标函数的梯度下降自然地学习语义上有趣的特征，只要任务需要这样的特征。我们可以了解男性和女性之间的区别，或者关于眼镜的存在或不存在，而不必通过涵盖所有这些值组合的示例来表征n-1个其他特征的所有配置。这种形式的统计可分离性使得人们能够推广在训练期间从未见过的人的特征的新配置。###15.5（从深度获得指数级增益）Exponential Gains from Depth在章节6.4.1中我们已经看到，多层感知器是通用逼近器，并且一些函数可以由比浅层网络相小指数级的深层网络表示。模型尺寸变小会提升统计效率。在本节中，我们将描述类似结果如何更一般地推广到其他种类的包含分布式隐藏表征的模型。
在章节15.4，我们看到一个生成模型的例子，该模型学习了人脸图像的解释性因素，包括人的性别以及他们是否戴眼镜。完成该任务的生成模型是建立在一个深度神经网络之上的。期望浅层网络（例如线性网络）学习这些抽象解释因素与图像中像素之间的复杂关系是不合理的。在这个和其他AI任务中，生成数据的因素几乎可以独立地选择，来生成更可能是非常高级的并且与输入高度非线性相关的数据。我们认为这需要深度分布式表征，即通过组合许多非线性（特征）获得较高级特征（被视为输入的函数）或因子（被视为生成原因）。
在许多不同的情况中已经证明，通过使用许多非线性的组合和重用特征的分层结构来组织计算，可以在使用分布式表征给出的指数提升之上给予统计效率带来指数提升。具有单个隐层的许多种网络（例如，具有饱和非线性，布尔门，和/乘积或RBF单元）可以被示为通用近似器。作为通用的近似模型族可以在给定足够隐单元的情况下逼近大类函数（包括所有连续函数）直到任何非零容限级别。然而，所需隐单元的数量可能非常大。有关深层架构表达能力的理论研究结果表明，存在函数族可以通过深度为k的架构有效地表示，但是在深度不足（深度为2或深度为k-1）时将需要指数级别的隐单元数目（相对于输入大小）。
在章节6.4.1，我们看到确定性前馈网络是函数的通用近似逼近。 许多结构化概率模型包含具有隐变量的单一隐层，包括受限波尔兹曼机和深度置信网络，是概率分布的通用近似逼近 (Le Roux and Bengio, 2008, 2010; Montúfar and Ay, 2011; Montúfar, 2014; Krause et al.,2013) 。
在章节6.4.1中，我们看到足够深的前馈网络相比浅层网络具有指数级别的优势。其他模型也可以获得这样的结果，例如概率模型。一个这样的概率模型是和积网络，或称为SPN（Poon和Domingos，2011）。这些模型使用多项式电路来计算一组随机变量的概率分布。 Delalleau和Bengio（2011）研究表明存在概率分布，其中需要SPN的最小深度来避免指数级别的大模型。后来，Martens和Medabalimi（2014）的研究表明，SPN的每两个有限深度之间存在显著差异，其中使SPN可控的一些约束可能限制其表达能力。
另一个有趣的发展是，对于与卷积网络相关的深度电路族的表达能力的一组理论结果，突出了深度电路的指数优势，即使当浅电路仅被允许近似深度电路计算的函数（Cohen et al.,2015）。相比之下，以前的理论工作只针对浅电路必须准确复制特定功能的情况提出了要求。###15.6（提供线索来发现根本原因）Providing Clues to Discover Underlying Causes我们以最初的问题之一来结束本章：什么使得一个表征比另一个更好？在章节15.3中首次介绍过的一个答案是，一个理想的表征形式，能够产生数据变化的潜在因素，特别是那些与我们的应用相关的因素。表示学习的大多数策略是基于引入线索，帮助学习找到这些潜在的变化因素。线索可以帮助学习者将这些观察到的因素与其他因素分离。监督学习提供了非常强的线索：每个x呈现一个标签y，其通常直接指定变化的至少一个因素的值。更一般地，为了利用丰富的未标记数据，表征学习利用关于基本因素的其它非直接的暗示。这些提示采用隐含的先验的信念形式，我们，学习算法的设计者强加，以指导学习者。诸如没有免费午餐定理的结果表明，正则化策略对于获得良好的泛化能力是必要的。虽然不可能找到一个普遍优越的正则化策略，深度学习的一个目标是找到一套相当通用的正则化策略，适用于各种各样的AI任务，类似于人和动物能够解决的任务。
我们在这里提供这些通用正则化策略的列表。 该列表显然不是详尽的，但是给出了一些具体的例子，可以鼓励学习算法发现对应于基本因素的特征。这个列表在Bengio et al. (2013d) 中章节3.1，并在这里加以扩充。
*	一、平滑度（Smoothness）：这个假设是对于足够的额单位d，有f（x + d）≈f（x）。这个假设允许学习者从训练样例推广到输入空间中附近的点。许多机器学习算法利用这个想法，但它不足以克服维数灾难。*	二、线性（Linearity）：许多学习算法假设一些变量之间的关系是线性的。这允许算法对远离观察到的数据进行预测，但有时可能导致过度极端的预测。最简单的机器学习算法，不做平滑假设，而是做线性假设。这些实际上是不同的假设 -应用于高维空间的具有大权重的线性函数可能不是非常平滑的。参见Goodfellow et al. (2014b)进一步讨论线性假设的局限性。*	三、多个解释因素（Multiple explanatory factors）：许多表征学习算法是基于数据由多个潜在的解释因素产生的假设，并且给定每个这些因素的状态大多数任务可以容易地解决。章节15.3描述了这种观点如何通过表示学习来激发半监督学习。学习p（x）的结构需要学习一些对建模p（y | x）有用的相同特征，因为它们指的是相同的基本解释因素。章节15.4描述了该视图如何激励分布式表征的使用，其中表示空间中的单独方向对应于单独的变化因子。*	四、因果因素（Causal factors）：模型是以这样的方式构建，使得其将由学习表示h描述的变化因素作为观察数据x的原因，而不是反之亦然。如章节15.3，这对于半监督学习是有利的，并且当基础原因的分布改变时或当我们将模型用于新任务时，使得学习模型更加鲁棒。*	五、深度，或者解释性因素的层次结构（Depth, or a hierarchical organization of explanatory factors）：高级别的抽象概念可以用简单的概念定义，形成层次结构。从另一个角度来看，深层架构的使用表达了我们的信念，即任务应该通过多步骤程序完成，每个步骤返回到通过先前步骤完成的处理的输出。*	六、跨任务的共享因子（Shared factors across tasks）：在我们具有许多任务的上下文中，其对应于共享相同输入x的不同yi变量或其中每个任务与全局输入x的子集或函数f（i）（x）相关联，假设每个yi与来自相关因子h的公共池的不同子集相关联。因为这些子集重叠，所以通过共享中间表示P（h | x）学习所有P（yi | x）允许在任务之间共享统计强度。*	七、流形（Manifolds）：概率质量集中，其中集中的区域在本地连接并占据很小的体积。在连续情况下，这些区域可以通过具有比数据所存在的原始空间小得多的维度的低维流形来近似。许多机器学习算法仅在这个歧管上行为合理（Goodfellow等人，2014b）。一些机器学习算法，特别是自动编码器，试图明确地学习歧管的结构。*	八、自然聚类（Natural clustering）：许多机器学习算法假定输入空间中的每个连接的流形可以分配给单个类。数据可以位于许多断开的歧管上，但是类别在这些中的每一个中保持恒定。这个假设激励了各种学习算法，包括切线传播，双反向推理，歧管切线分类器和对抗训练。时间和空间相干（Temporal and spatial coherence）：慢特征分析和相关算法使得最重要的解释因素随时间缓慢变化的假设，或至少预测真实潜在解释因素比预测诸如像素值的原始观察更容易。见章节13.3中这种方法的进一步描述。*	九、稀疏性（Sparsity）：大多数特征应该与描述大多数输入不相关 - 在表示猫的图像时，不需要使用检测大象树干的特征。因此，合理的是强加一个先验，即可解释为“存在”或“不存在”的任何特征在大多数时间应该不存在。*	十、因子依赖的简单性（Simplicity of Factor Dependencies）：在好的高级表示中，因子间通过简单的依赖关系相互联系。最简单的可能就是边际依赖，但线性依赖性或由浅自动编码器捕获的那些（依赖）也是合理的假设。 这可以在许多物理定律中看到，并且当在学习的表示的之上加入线性预测器或因式分解的先验时被假设。表示学习的概念将所有的深度学习形式联系在一起。 前馈和反复网络，自动编码器和深度概率模型都学习和利用表示。 学习最好的表示仍然是一个令人兴奋的研究的途径。