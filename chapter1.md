###第一章介绍
发明家一直梦想可以造出能够思考的机器。这样的想法一直可以追溯到古希腊时期。神话人物皮格马利翁，代达罗斯和赫淮斯托斯都可以被解释为传说中的发明家，而加拉蒂亚，塔洛斯，和潘多拉则可以被看作是人工智能生命(Ovid and Martin, 2004; Sparkes, 1996; Tandy, 1997)。在可编程计算机问世的百余年前，人们在首次设想这样的机器时就好奇他们是否会变得智能(Lovelace, 1842)。如今人工智能这个蓬勃发展的领域已经有了很多实际应用和热点研究课题。我们期待人工智能软件能够处理日常工作，理解语音或者图像，完成医学诊断和支持基础科学研究。在人工智能的早期，一些领域的问题被迅速解决，这些领域的特点是对于人们来说非常困难，但是可以被表达成一系列的数学规则，从而使得计算机可以直观地解决。而人工智能面临的正真困难是解决一些很容易被人们直观地解决但是难以正式描述的问题，例如识别交谈或者图像中的人脸。本书正是关于解决更多直观问题。该解决方案是为了让计算机从经验中学习，和通过层次化的概念理解世界，其中每个概念通过与其他简单概念的关系来定义。通过从经验获取知识，这种方法避免了人工操作员正式指定计算机需要的所有知识。层次化的概念允许计算机通过学习简单概念来构建复杂的概念。如果我们画图来表示层次化的概念，那么这张图会有很深的多层结构。出于这个原因，我们称之为AI深度学习。在人工智能的早期，许多成功发生于相对正式和没有干扰的环境中，并不需要计算机具备真实世界的相关知识。例如，1997年IBM的深蓝国际象棋系统击败了世界冠军卡斯帕罗夫（Hsu，2002年）。国际象棋当然是一个很简单的世界，只包含64个位置和32枚只能在严格规则限制下移动的棋子。成功设计出国际象棋的策略是巨大的成就，但是面临的挑战却不是向计算机描述棋子和其所允许的移动。因为国际象棋可以由完全形式化的规则描述，并由程序员提前提供。然而讽刺的是，抽象和形式化任务对人的思考是最困难的，但对于计算机却是非常简单的。计算机早已能够打败人力最好的国际象棋选手，但是直到最近才匹敌人们识别物体或者语言的平均水平人们。人们的日常生活需要大量关于世界的知识，然而这些知识往往是主观和依靠直觉的，所以难以用正式的方式表达。计算机如果想智能地作出行为就必须抓住同样的知识。所以人工智能的一个关键挑战就是如何让计算机了解非正式的知识。一些人工智能项目力求通过形式化的语言将客观世界的知识进行硬编码，这样计算机就可以使用逻辑推理规则来自动地推理这些使用形式化语言的表达，这就是基于知识的人工智能。其中一个最著名的项目就是Cyc (Lenat and Guha, 1989)，Cyc由一个推理引擎和基于CYCL语言描述语句的数据库构成。数据库由专门的工作人员输入描述语句，这个过程非常笨拙。人们很难想出足够复杂的正式规来准确地描述世界。例如，Cyc没能理解一个名叫Fred的人在早晨剃须的故事(Linde, 1992)。Cyc的推理引擎检测到了故事中的矛盾，它知道人不可能由电子零件构成，但是它相信在刮胡子的Fred包含电子零件，因为Fred拿着剃须刀。所以它发出了疑问，Fred在剃胡子的时候是否还是一个人。依靠将知识进行硬编码的系统所面临的困难表明，AI系统需要掌握通过原始数据获取自己的知识的能力。这种能力被称为机器学习。机器学习知识的引入使得计算机能够解决涉及现实世界知识的问题，并作出主观决策。通过简单的名为逻辑回归的机器学习算法就可以决定是否建议剖宫产(Mor-Yosef et al., 1990)，而名为朴素贝叶斯的机器学习算法可以从垃圾邮件中识别合法的邮件。这些简单机器学习算法的性能在很大程度上依赖于它们被给定数据的表达形式。例如，当逻辑回归算法被用于是否建议剖宫产时，AI系统并不直接检查病人，而是由医生告诉其相关的信息，例如病人是否存在子宫疤痕。描述患者的每部分信息被称为一个特征。逻辑回归算法学习了这些特征与诊断结果的相关性，然而算法并不能明确特征如何定义。如果算法不接受医生输入的正式报告，而是直接处理病人的MRI扫描图像，算法就不能够给出有用的预测结果。MRI图像的中的每个像素与任何并发症间的相关性都是微不足道的。对于表达形式的依赖是在整个计算机科学乃至日常生活中出现的普遍现象。在计算机科学中，如果在结构化和有索引的数据集中寻找指定数据，这样的操作可达指数级别的速度。人们可以很容易地通过罗马数字进行运算，但换用罗马数字就会慢很多。所以数据的表达形式能够对机器学习算法的性能产生巨大影响也就不足为奇了。举个简单的直观例子，如图1.1所示。（图1.1 数据的不同表达形式：例如我们希望通过绘制一条直线将散点图中的两类数据分开。左图中我们使用了笛卡尔坐标，任务是不可能完成的。右图中，我们使用了极坐标，任务变得非常简单，用一条垂直的线即可解决。）许多人工智能的任务都可以通过设计适用于任务的特征集提取，然后将这些特征输入简单的机器学习算法来解决。例如，说话者声音识别的一个有用特征是发声者声道大小的估计，因为这个特征为确定发声者是男人，女人还是小孩提供了强线索。然而对于许多任务很难确定需要提取哪些特征。例如。我们想要编写一个程序来检测照片中的汽车。我们知道汽车有轮子，所以我们可能会想用车轮的存在作为特征。然而不幸的是，很难通过像素值来准确地描述车轮的样子。车轮本身具有简单的几何形状，但是它的图像可能因为很多情况变得复杂，例如被阴影遮挡，其上的金属因阳关而耀眼，被汽车的挡泥板或者其他物体遮住一部分等等。解决这个问题的一个方法是不只用机器学习算法来学习从数据表达到输出，同样学习如何表达。这种方法被称为表征学习。学习而得的特征表达往往能获得比人工设计特征更好的性能。他们还允许AI系统以最少的人工干预来迅速适应新的任务。表征学习算法可以在几分钟内发现简单任务的特征集，复杂任务可能需要数小时甚至数月。人工为复杂任务设计特征需要投入很多时间和精力，甚至可能需要整个研究组织投入几十年的时间。一个代表性的表征学习算法就是Auto encoder。Auto encoder由多个编码函数和解码函数构成，其中编码函数能够将输入数据转换成另一种不同的表达形式，而解码函数能够将新的表达转化成原始格式。Auto encoder的训练目标不仅是，在输入经由解码器到编码器时尽可能保留信息，同时获得新的有较好特征的数据表达形式。不同种类Auto encoders的目标是实现不同种类的属性。当设计特征或者学习特征的算法时，我们的目标通常是区分那些能够解释观测数据的变化因素。在此背景下，我们使用“因素”这个词是为了区分不同来源的影响，这些因素通常不需要乘法组合而得。这些因素往往不是可以直接观察到的数量值。相反，他们可能以不可观察的物体或不可观察的力量存在于现实世界中从而影响观察到数量值。他们还可能以结构化的形式存在于人脑中，提供有用的观测数据的简化解释或推测原因。这些可以被看做是帮助我们从数据变化中获得有意义推论的概念或是抽象。当分析语音片段时，这些变量因素包括发声者的年龄，性别，和所说的词汇。当分析汽车图像时，这些变量因素包括汽车的位置和颜色，阳光的强度和角度。许多现实世界人工智能应用面临的主要问题是，我们能够观测到的数据受到很多因素变化的影响。例如，一幅图像中的红色汽车在夜晚看起来非常接近黑色，而汽车的轮廓取决于观察者的视角。大多数应用需要我们去理清这些变化的因素并剔除其中我们不关心的。当然，从原始数据中提取这样的高阶抽象特征是非常困难的。许多这样的变化因素只能通过复杂的接近人类水平的理解能力来识别，例如发声者的口音。当几乎很难获得解决原始问题的特征表达时，表征学习看起来可以帮助我们解决问题。 深度学习通过引入使用简单表征构成复杂表征的方法解决如上的中心问题。深度学习允许计算机通过简单的概念来构建复杂的概念。图1.2显示了深度学习系统如何通过组合简单的概念来表征图像中的人，例如通过边缘定义的角落和轮廓。这个深度学习的典型例子即前馈深度网络或多层感知器（MLP）。多层感知器仅仅是一个由输入到输出的数学函数映射，这个函数由多个简单函数构成，我们可以把不同的应用理解成不同的数学函数，每个函数可以实现输入的新的表征方式。学习数据正确表征方式的想法为深度学习提供了一个切入点，另一个切入点是深度学习允许计算机学习得到一个包含多步骤的计算程序。每一层特征表达可以理解为计算机在并行过程中执行一组指令后的存储状态。深层网络可以系列化地执行更多指令。由于后续指令可以回溯前序指令的结果，序列化的指令因而具备更强的能力。根据深度学习的观点，并不是单层的所有激活信息都会编码输入的变化因素，每层的表示还会存储状态信息来帮助程序理解输入。这些状态信息类似于传统计算机程序的计数器或指针，它与具体内容无关，但是却能够帮助模型组织整个处理过程。测量模型的深度有两种主要方法，第一种方法是基于评估架构必须执行的序列化指令的数量。我们可以认为这是描述如何从给定模型输入计算输出的流程图中最长路径的长度。正如两个相同的程序，以不同的编程语言实现会有不同的长度，相同的功能也会具有不同的深度，取决于流程图中各个步骤我们使用的子功能。图1.3说明了相同架构在不同语言选择下具有不同的描述。另一种被深度概率模型使用的方法是，模型的深度使用描述概念间相互联系的图的深度，而不是计算流程图的深度。在这种情况下，表示计算概念间相互联系流程图的深度可以比概念本身（流程图）更深。这是因为系统对简单概念的理解可以改进给出更复杂的概念。例如，AI系统在观察一只眼睛被阴影遮挡的面部图像时可能最初只看到一只眼睛。在检测到人脸后，可以推断第二只眼睛也存在。这种情况下，概念图只包含两层，眼睛和人脸，但是计算图将包含2N层，如果我们在处理每个概念时作出N次改进的估计。

因为不同的人会选择不同的最小元素来构建图模型，所以到底是计算图的深度，还是概率模型图的深度，这两种观点到底哪一种才能符合实际并不总是很清楚。正如计算机程序并没有确切长度一样，一个架构的深度也没有一个单一的正确值。模型需要具有什么样的深度才能称之为“深”并没有共识。但是深度学习通常可以被认为是涉及比传统机器学习更多的函数或者概念的研究。总之，本书的主题深度学习是一种人工智能方法。具体地讲，这种机器学习技术允许计算机系统通过经验和数据进行演化提升。据本书的作者，机器学习是在复杂真实世界环境中构建AI系统的唯一可行方法。而深度学习这种特定类型的机器学习方法，通过层次化嵌套的简单概念，和由简单抽象定义的更抽象标准，获得了更强更灵活的表征世界的方法。图1.4说明了这些不同AI学科间的关系。图1.5给出了这些学科工作的高层次原理。

####1.1需要阅读本书的读者本书对很多读者都会起到帮助作用，但是我们主要考虑两个目标受众。一是正在学习机器学习的大学生（本科生或者研究生），也包括那些以深度学习和人工智能研究为方向的职业人。另一个是那些没有机器学习或统计知识背景，但是想要迅速获取相关知识并在平台上使用深度学习技术的软件工程师。深度学习已经在很多领域产生了应用，包括计算机视觉，语音和音频处理，自然语言处理，机器人技术，生物信息学，化学，电子游戏，搜索引擎，网络广告和金融等。本书编排成了三个部分以最好地适应不同的读者。第一部分介绍了基本的数学工具和机器学习基础概念。第二部分介绍了最完善的研究最彻底的深度学习算法。第三部分介绍了被广泛认为是未来深度学习最重要研究方向的新热点。读者可以随意跳过自己不关心或者与自身背景不匹配的章节。熟悉线性代数，概率，和基本的机器学习概念的读者可以跳过第一部分。只想实现可以工作的系统的读者可以略过第二部分。图1.6展示了本书的组织结构来帮助读者选择阅读哪个章节。我们假设所有的读者都具有计算机科学背景，对编程，计算机性能问题，计算复杂度，微积分和图理论有基本的了解。####1.2深度学习的历史趋势了解一些历史背景有助于理解深度学习。我们仅仅提及一些关键趋势，而不提供深度学习的详细历史。a）深度学习本身具有丰富悠久的历史，但是从不同的角度出发有很多不同名字，所以流行度有衰减趋势。b）随着训练数据的增加，深度学习的效果越来越好。c）随着计算机硬件和深度学习软件基础架构的改善，深度学习模型的规模越来越大。d）随着时间的推移，深度学习解决复杂应用的精度越来越高。

#####1.2.1神经网络的多舛命运和众多得名
我们预期本书的很多读者都听说过深度学习这种激动人心的新技术，并惊讶地看到本书竟然提及一个新兴领域的“历史”。事实上，深度学习的历史可以追溯至20世纪40年代，因为在其流行前经历了许多遇冷的年份，而且在新得名“深度学习”前有众多名字，所以才显得新颖。这个研究领域几经易名，反映了不同研究人员的多种观点。深度学习的历史综述超出了本书的范围。然而，了解一些基本背景对于理解深度学习十分有益。一般来讲，深度学习有三次发展浪潮：深度学习在20世纪40年代至60年代被称为控制论；在上世纪80年代至90年代被誉为联结学习；从2006年开始以深度学习这个名字开始复苏。图1.7中是量化的表示。一些早期的算法在我们今天看来是实际是生物学习的计算模型，即大脑中已经发生或未发生的学习过程。所以，深度学习的一个过了时的名字即人工神经网络（ANNs）。而深度学习中对应的观点是，这样的模型是受生物大脑启发而构建的工程系统（无论人类大脑或其他动物的大脑）。机器学习中的神经网络有时被用于理解大脑功能(Hinton and Shallice, 1991) ，然而他们一般都没有被设计成具有实际生物功能的模型。深度学习中的神经元思想主要受两个观点启发。一个观点是，大脑为智能行为提供了例证，和通过对大脑计算原理进行逆向工程、复制其功能来构建智能的直接路径。另一个观点是，深刻理解大脑和其背后的人类智力将会非常有趣，所以除了解决工程问题的能力以外，机器学习模型对于阐明上述基本科学问题也是非常有益的。从机器学习模型构建的角度来说，现代术语“深度学习”已经超越了神经科学的范畴。它展现出了学习多层次组合的更普遍原理，而不仅局限于应用在基于神经学的机器学习框架。现代深度学习的最早前身是受神经科学启发而构建的简单线性模型。这些模型被设计为采用一组n个输入值X1， ...， Xn，并将它们与一个输出y相关联。这些模型将会学习出一组权重系数，W1... ，WN，和计算它们的输出函数f（x ， w）= x1w1 + ··· + xnwn 。第一波研究神经网络的浪潮被称为控制论。如图1.7所示。麦卡洛克 - 皮茨神经元（麦卡洛克和皮茨， 1943年）是模仿大脑功能的早期模型。该线性模型可以通过测试函数f（x ， w）的正负来识别两种不同类型的输入。当然需要设置正确的权重来保证模型可以响应预设的分类。这些权重可以人为设置。在20世纪50年代，感知器(Rosenblatt, 1958, 1962) 成为了第一个可以从各类别样本中学习出权重的模型。同一时期的自适应线性元件（ ADALINE ），可以简单地返回函数f（x ）的值本身来预测一个实数(Widrow and Hoff, 1960) ，并且还可以学习到从数据预测这些数字。这些简单算法对现代机器学习产生了巨大的影响。用于调节ADALINE的权重训练算法是随机梯度下降算法的一个特例。在此基础上微加修改的随机梯度下降法仍然是当今训练深度学习算法的主流方法。感知器和ADALINE使用的基于决策函数的模型被称为线性模型。尽管在许多情况下，这些模型的训练方式在原始方法上进行了改进，但仍然是被最广泛使用的机器学习模型。

线性模型有很多局限性。最著名的是，它们不能学习XOR函数，其中f （ [0,1] ，W） = 1和f（ [1,0] ，W） = 1，但是F（ [1,1] ，W ）= 0和F（ [0,0] ，W ）= 0 。发现线性模型中这些缺陷的批评者引发了对生物启发学习的反弹(Minsky and Papert, 1969)。这是神经网络流行潮的第一次跌落。今天，神经科学被视为深度学习研究的一个重要灵感来源，但它已不再是该领域的主要导向。目前神经科学在深度学习研究中的作用被削弱，主要的原因是我们根本没有关于大脑的足够信息来使用它。我们需要能够监测（至少是）数千互联神经元的同时活动，从而获得大脑实际如何使用算法的深刻理解。因为我们不能够做到这一点，我们还远远没有理解大脑的最简单或最充分研究的部分(Olshausen and Field, 2005)。神经科学家已经给了我们一个理由去希望一个深度学习算法可以解决不同的任务。神经学家们发现，如果将雪貂大脑的听觉处理区域重新连接传送视觉信号，该区域可以学会去“看”（Von Melchner等， 2000）。这表明哺乳动物的大脑可能使用同一算法来解决不同需要处理的任务。在这个假说之前，机器学习的研究很分散，不同研究组织的人员研究不同的领域，自然语言处理，机器视觉，运动规划和语音识别。目前，这些研究组织仍然是分散的，但是一个深度学习研究小组同时研究多个或者所有这些应用领域已经非常常见。我们能够从神经学得到一些粗略的指南。仅仅通过计算单元间的相互作用就能变得智能的想法是受大脑启发。一款名为Neocognitron (Fukushima, 1980) 的功能强大的图像处理架构是受哺乳动物视觉系统启发，而这后来成了现代卷积神经网络的基础，在9.10有更详细的介绍。目前，神经网络大多基于ReLU，最初的Cognitron受人脑功能启发引入了非常复杂的神经元模型。而现代简化版本的由来受到了很多观点的启发，Nair and Hinton (2010)和Glorot et al. (2011a) 引入援引了神经科学的观点，而Jarrett et al. (2009) 则援引了更多工程方面的影响。虽然神经科学是灵感的重要来源，但是它不需要被视为刚性指导。我们知道，实际的神经元相比现代的ReLU单位有不同的计算方式，但更好的神经元现实并未导致机器学习性能的改进。此外，神经科学已经成功启发构建了新的神经网络架构，但是我们还没有足够了解生物学习的神经科学，提供我们学习算法的指导来训练这些架构。媒体报道经常强调深度学习的与大脑相似。虽然这是事实，相比于其他机器学习领域，如核机器或贝叶斯统计，深度学习领域的研究人员更倾向于援引大脑相关的知识，但是人们不应该将深度学习看做是模拟大脑的尝试。现代深度学习受很多领域的启发并得到灵感，特别是应用数学，线性代数，概率论，信息论和数值优化等。部分深度学习研究人员引用神经科学并将其做为灵感的重要来源，然而也有人并不关心神经科学。值得注意的是，去了解大脑如何在算法层面工作的努力是一直存在并很好地展开。这样的努力被称为“计算神经科学”，且与深度学习是不一样的研究领域。研究人员经常从这两个领域之间流动是很常见的。深度学习领域内主要关注如何构建能够成功解决需要智能的任务的计算机系统，而计算神经科学领域主要关注建筑大脑如何工作的更精确模型。20世纪80年代，神经网络研究掀起了第二次浪潮，主要得益于连接或并行计算。(Rumelhart et al., 1986c; McClelland et al., 1995). 连接起源于认知科学，认知科学结合多个不同层次的分析，跨学科研究来理解大脑。在20世纪80年代初期，大多数认知科学家研究的是符号推理模型。尽管这些研究非常流行，符号模型很难解释大脑如何在实际使用中用神经元来解释他们。连接主义者开始研究可能在神经上实现的认知模型(Touretzky and Minton, 1985) ，复现了很多基于心理学家唐纳德·赫布在20世纪40年代工作的想法。连接主义的中心思想是，大量的简单计算单元通过网络连接在一起可以实现智能行为。这种见解同样适用于生物神经系统的神经元，和在计算模型中的隐单元。上世纪80年代，连接主义产生过程中的一些核心概念仍然是当今深度学习和核心思想。

这些概念中的一个就是分布式表达distributed representation (Hinton et al., 1986) 。这个想法说的是系统的每一个输入都应该由许多特征来表达，并且每个特征应该通过许多可能的输入来表示。例如，假设我们有一个视觉系统能够识别汽车，卡车，和鸟类，这些对象可以各自是红色，绿色，或蓝色。表示这些输入的一个方法是，用一个单独的神经元或隐单元来激活这九个可能的组合：红色的卡车，红色的轿车，红色的鸟，绿色的车等等。这需要九个不同的神经元，并且每个神经必须独立地学习的色彩和对象身份的概念。一个改进的方法是使用分布式表示，即用三个神经元描述颜色和三个神经元描述对象的身份。这仅需要六个神经元而不是九个，而且描述红色的神经元能够从不同的对象学习红色，如汽车，卡车和鸟类的图像，而不是​​特定类别的图像。分布式表达是这本书的核心概念，将会在第15章进行更为详细的说明。连接主义的另一个成就是成功使用BP算法来训练深度神经网络，和BP算法的普及。(Rumelhart et al., 1986a; LeCun, 1987)。该算法的流行跌宕起伏，但是截至本书完成时，BP算法仍然是训练深度模型的主流方法。在20世纪90年代，研究人员利用神经网络建立序列模型取得了重要进展。Hochreiter (1991) 和Bengio et al. (1994) 从数学角度确认了建立序列模型的困难，见10.7。Hochreiter和Schmidhuber (1997) 引入了LSTM模型去解决上述困难。今天， LSTM被广泛地用于许多序列建模任务，包括谷歌公司的许多自然语言处理任务。神经网络研究的第二次浪潮一直持续到90年代中期。基于神经网络等人工智能技术的企业在寻求投资的时候做出了不切实际的宣言。当人工智能研究没有实现这些不合理的期望时，投资者感到非常失望。同时机器学习的其它领域取得了进步。核函数方法和图模型都在许多重要任务上获得了好的结果。上述两个因素导致了神经网络在2007年前不再流行。在此期间，神经网络在一些任务上获得了令人印象深刻的表现(LeCun et al., 1998b; Bengio et al., 2001)。加拿大高级研究所（ CIFAR ）通过其神经计算和自适应感知（ NCAP ）研究计划来保持神经网络的研究活跃。这个联合的机器学习研究组织由伦敦大学的Geoffrey Hinton，蒙特利尔大学的Yoshua Bengio 和纽约大学的Yann LeCun领导。CIFAR的NCAP研究计划的多学科性质的，其中还包括在人类和计算机视觉专家和神经科学家。
在这个时间点上，深度网络的训练普遍被认为是非常困难的。我们现在知道，自从20世纪80年代已存在的算法工作得非常好，但是直到大约2006年才变得明显。或许仅仅是因为在当时可用硬件的条件下，这些算法的计算复杂性太高以至于不能完成充分的实验。神经网络研究的第三次浪潮开始于2006年的一个突破。Geoffrey Hinton发现深度置信网可以通过逐层贪心预训练的策略来有效地训练，在15.1将有更详细的介绍。CIFAR下属的其他研究小组很快发现同样的策略可以用来训练多种深度网络，并且帮助系统提升在训练集上的泛化表现。神经网络研究的这波浪潮使用深度学习这个词，来强调研究人员现在可以比原来训练更深的神经网络，并把注意力集中于深度这个概念(Bengio and LeCun, 2007; Delalleau and Bengio, 2011; Pascanu et al., 2014a; Montufar et al., 2014)。这时，深度神经网络获取了优于其他基于机器学习技术及人工设计函数的AI系统的表现。虽然深度学习研究的重点这期间内发生了巨大变化，神经网络的第三次浪潮至编写本书时仍在继续。第三次浪潮开始把重点放在新的无监督学习和从小数据获得出色泛化能力的技术上，但是目前更多的研究兴趣仍然集中在传统机器学习算法和深度学习算法在大规模有标数据上的能力。
#####1.2.2数据集膨胀
人们可能会好奇，为什么50年代就进行首次试验的人工神经网络，只是在最近才被认为是关键技术。深度学习从90年代已经成功商用，但往往被视为一种艺术而非技术的东西，只有专家才可以使用，直到最近才有所变化。深度学习算法想获得很好的性能确实需要一些技巧，幸运的是随着训练数据的增加技术人员所需的工作量正在减少。深度学习算法在一些复杂任务上的表现与人类相同，正如在20世纪80年代努力解决的一些简单问题，尽管有些算法的训练模式发生了变化，简化了深度结构的训练过程。最重要的新进展是，今天我们可以提供这些算法取得成功所需的资源（训练数据）。图1.8展示了基准数据集的大小随着时间的推移明显增加。这种趋势是由社会的日益数字化驱动的。由于我们的活动愈加发生在计算机上，我们做什么就有愈多的记录。我们的计算机越来越多地联网在一起，使得更容易集中这些记录，并将它们加工组织成适合于机器学习应用程序的数据集。“大数据”时代下的机器学习变得容易，因为统计估计的负担已经被大大减轻，即仅观察少量后泛化到新的数据。截至2016年，一个粗略的规则是，一个深度监督学习算法在每个类别约5000标识样本的情况下，一般会达到可接受的性能；当在含有至少有1000万标识样本的数据集上训练时将达到或超过人的表现。如何在小数据上获得成功是一个重要的研究领域，特别是如何使用无监督学习或者半监督学习来利用大量的未标记样本。#####1.2.3模型尺寸膨胀
相比于20世纪80年代神经网络相对较少的成就，今天我们成功的一个重要原因是拥有可以运行更大模型的计算资源。连接主义的一个主要见解是，当很多神经元在一起工作时动物会变得聪明，而单独神经元或者少数神经元集合不是特别有效。生物神经元没有特别密集地连接。如图1.10 所示，我们的机器学习模型中每个神经元都有数个连接，这是哺乳动物大脑在几十年的进化中建立的连接。就神经元的总数目而言，直到最近神经网络的尺寸都小的惊人，如图1.11 所示。自从引入隐单元开始，神经网络的尺寸每2.4年增加一倍。这种增长是由内存更大速度更快的计算机和更大的数据集驱动的。较大的网络能够在更复杂的任务上获得更高的精度，这种趋势看起来将持续数十年。除非新技术允许更快采样，人工神经网络至少到21世纪50年代才能与人脑具有相同数量的神经元。生物神经元功能比目前的人工神经元更为复杂，使生物神经网络可能比上述描绘的更大。回首这个过程，比水蛭拥用神经元数量还少的神经网络不能解决复杂的人工智能问题，是不足为奇的。即使在今天的神经网络，这从计算系统的角度我们认为它相当大，但是仍比原始脊椎动物的神经系统小得多，例如青蛙。由于更快的CPU和通用GPU的使用，更快的网络连接，和用于分布式计算的更好的软件基础设施，模型的尺寸会随着时间推移而增加，这也是深度学习的最重要趋势之一。这个趋势通常被认为将持续。
#####1.2.4 持续提升的准确率，复杂度以及对现实世界的影响从80年代以来，深度学习提供准确识别或者预测的能力一直在改善。此外，深度学习在越来越多的领域被成功应用。最早的深度学习模型被用来识别分割出的非常小的单个图像对象(Rumelhart et al., 1986a) 。从彼时开始，神经网络能够处理的图像尺寸逐渐增加。现代物体识别网络可以处理高分辨率的图片，并不需要待处理图片中的物体位置有特殊要求(Krizhevsky et al., 2012).。同样地，早期神经网络只能识别两种对象（或者在某种情况下，一种对象不存在或者仅存在一种对象），而现代网络通常至少能够识别1,000个不同类别的对象。规模最大的目标识别大赛是每年举行的ImageNet大型视觉识别挑战（ILSVRC）。当卷积神经网络以15.3％的前5错误率大幅赢得这一比赛时（Krizhevsky等，2012），深度学习迎来了戏剧性的翻红，当时最好的前5错误率是26.1％。这意味着卷积网络为每个图像生成的有序可能类别中，只有15.3％的测试样本没有出现在这个列表的前五名。此后，深度卷积网络持续赢得了这些比赛，截至本书开始编写时，深度学习的进步使得最新的前5错误率在本次比赛中下降到3.6％，如图1.12所示。 深度学习也对语音识别产生了巨大影响。在整个20世纪90年代后的进步后，语音识别的错误率停滞不前，直至2000年左右深度学习的引入。语音识别在引入了深度学习后迅速降低了错误率，某些情况下错误率甚至减半。我们将在章节12.3讨论这段历史的更多细节。深度学习网络在行人检测和图像分割领域也有惊人的成就(Sermanet et al., 2013; Farabet et al., 2013; Couprie et al., 2013) ，并在交通标志分类中有惊人表现(Ciresan et al., 2012)。在深度网络的尺寸和精度提升的同事，网络能够解决复杂问题的能力也在提高。Goodfellow et al. (2014d) 的研究表明，神经网络可以学习到输出从图像转录字符的完整序列，而不是仅仅识别一个对象。此前人们普遍认为，这种学习需要(Gülçehre and Bengio, 2013)标记序列中的单个元素。递归神经网络，如前文所述的LSTM序列模型，现在用于建模序列和其他序列之间的关系，而不仅仅是固定的输入。这种序列到序列的技术似乎是处在革命其他应用程序的风口浪尖：例如机器翻译(Sutskever et al., 2014; Bahdanau et al.,2015)。神经网络日趋复杂的趋势为其带来了逻辑定论，即神经图灵机(Graves et al., 2014a) ，这样的机器可以读取存储单元的内容和写入任意内容到存储单元。这样的神经网络可以通过样例来学习简单的程序。例如它们可以学习给定的乱序和排序实数列表来进行排序。这种自我编程技术正处于起步阶段，但未来在原则上几乎可以适用于所有的任务。深度学习的另一个最大成就是其推广其到强化学习的领域。在强化学习的背景下，一个独立的代理必须学会在未经人为操作的任何指导下，通过试错来执行任务。 DeepMind表明，基于强化学习的深度学习系统能够学会玩雅达利视频游戏，在许多任务上达到人类的表现(Mnih et al., 2015) 。深度学习也显著改善了机器人在强化学习中的表现(Finn et al., 2015)。深度学习的许多应用都有利可图。深度学习现在被许多顶级的技术公司使用，包括谷歌，微软，Facebook和IBM，百度，苹果，Adobe公司，Netflix公司，NVIDIA和NEC。深度学习的进展也得益于软件架构的进步。软件包对研究项目和商业产品都起到了支持作用，例如Theano (Bergstra et al., 2010; Bastien et al., 2012), PyLearn2 (Goodfellow et al., 2013c), Torch (Collobert et al., 2011b), DistBelief (Dean et al., 2012), Caffe (Jia, 2013), MXNet (Chen et al., 2015), 和TensorFlow (Abadi et al., 2015) 。深度学习也对其他科学领域做出了贡献。用于物体识别的深度卷积模型为神经科学家提供了可以研究的视觉模型(DiCarlo, 2013) 。深度学习同样为海量数据处理提供了有用的工具，并且在科学领域做出了有效的预测。它已成功地用于预测分子间如何发生相互作用，来帮助制药公司设计新的药物；搜索亚原子粒子；并自动解析显微镜​​图像来构建人脑的三维图。我们预计深学习将在未来出现在越来越多的科研领域。总之，深度学习作为机器学习的一种方法，在过去几十年的发展过程中，大量利用了我们人脑的知识，统计学知识与应用数学。近年来，深度学习的通用性和实用性有了极大的发展，在很大程度上得益于更强大的计算机，更大的数据集和更好的网络训练技术。未来几年都充满了挑战，和进一步提高深度学习并把它带到新领域的机会。