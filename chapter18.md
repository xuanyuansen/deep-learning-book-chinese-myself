##第十八章：直面配分函数（Confronting the Partition Function）
===
章节16.2.2中，我们看到许多概率模型（通常是无向图模型）都是通过一个非归一化的概率分布P(x; θ)来定义的。我们需要通过将P除以一个配分函数Z(θ)来获得一个有效的概率分布：

配分函数是在变量所有状态的非归一化分布上的积分（连续变量）或者求和（离散变量）。


对于许多感兴趣的模型这个操作是难以处理的。
我们在第二十章中看到，一些深度学习模型被设计成有一个可以处理的归一化常量，或者被设计成只以不涉及计算P(x)的方式来使用。然而，其它模型要直接面对难处理的配分函数的挑战。在本章中，我们描述了用于训练和评估包含难处理的配分函数的模型的技术。
###18.1	对数似然梯度（The Log-Likelihood Gradient）
使最大似然学习无向图模型特别困难的是配分函数依赖于参数。相对于参数的对数似然的梯度包含对应于配分函数的梯度的项：

这是一个众所周知的分解，即变成正相和负相的学习。
对于大多数感兴趣的无向图模型，负相学习是困难的。没有隐变量或者隐变量之间几乎没有相互作用的模型通常具有易处理的正相学习。具有直接正相和困难负相的模型的典型示例是RBM，在给定可见单位的情况下其具有彼此条件独立的隐单元。在第19章中主要讨论了在隐变量之间具有复杂的相互作用的情况下，正相学习是困难的。本章着重讨论负相学习的困难。
让我们更进一步观察logZ的梯度。

对于保证所有x都有p（x）>0的模型，我们可以用exp（log p（x））代替p（x）：


这种推导使用对离散x的求和，但是类似的结果适用于对连续x的积分。在推导的连续版本中，我们使用Leibniz规则在积分符号下进行求导以获得等式，

这个等式只适用于某些规律性条件下的p和∂p(x)/∂θ。在量度理论术语中，条件是：（i）对于每个θ值，p必须是x的Lebesgue可积分函数;（ii）∂p（x）/∂θ必须对于所有θ和几乎所有的x都存在;（iii）必须存在界定∂p（x）/∂θ的可积分函数R（x）对于所有θ和几乎所有x有| ∂p（x）| ≤R（x））。幸运的是，大多数机器学习感兴趣的模型都具有这些属性。

这个恒等式是用于近似最大化具有难处理配分函数的模型的各种蒙特卡罗方法的基础。

使用蒙特卡罗方法来学习无向图模型提供了一个直观的框架，其中包括我们可以想到的正相和负相。在正相阶段，我们从数据中抽样的x增加log p（x）。在负相中，我们通过减少从模型分布中抽样的log p（x）来减小配分函数。
在深度学习文献中，通常以能量函数（方程16.7）来参数化log p。在这种情况下，我们可以将正相解释为减弱训练样本的能量，将负相解释为增强从模型中提取的样本的能量，如图18.1所示。

图18.1：算法18.1的视图具有“正相位”和“负相位”。（左）在正相位中，我们从数据分布中采样点，并推高它们非归一化的概率。这意味着可能在数据中的点被推高了。（右）在负相阶段，我们从模型分布中抽取点，并下推它们的非归一化概率。这抵消了正相阶段的趋势，只是在每一处添加一个大的常数到非归一化的概率。当数据分布和模型分布相等时，正相位具有相同的机会在一个点上推，因为负相位必须下推。 当这种情况发生时，不再有任何梯度（期望），训练必须终止。
###18.2	随机最大似然和对比散度（Stochastic Maximum Likelihood and Contrastive Divergence）
实现方程18.15的天然方法是，通过在每次需要梯度时从随机初始化中的一组马尔科夫链中进行计算来计算。当使用随机梯度下降执行学习时，这意味着链必须在每个梯度步骤中被燃烧一次。这种方法可以推出在算法18.1中呈现的训练过程。在内循环中的马尔科夫链中的高成本燃烧使得该过程在计算上不可行，但是该过程是其它更可行的算法旨在近似的起点。

我们可以看到MCMC方法的最大似然，试图实现两股力量之间的平衡，一个增强已知数据上的模型分布，另一个抑制模型样本上的模型分布。图18.1展示了该过程。这两股力对应于最大化log p和最小化log Z。 一些近似对于负阶段是可能的。这些近似中的每一个可以被理解为使得负阶段在计算上更容易，但也使得其在错误的位置会下降。

因为负阶段涉及从模型的分布中采样，我们可以认为它是模型认为强烈要寻找的点。因为负阶段用于降低这些点的概率，所以它们通常被认为是表示模型对世界不正确的信念。

它们在文献中经常被称为“幻觉”或“幻想粒子”。事实上，负阶段已经被提出作为在人类和其他动物中做梦的可能解释（Crick和Mitchison，1983），该想法是：大脑维护一个对世界的概率模型，并遵循log p的梯度，同时在清醒时经历真实事件，并遵循log p的负梯度以在睡眠和体验从当前模型采样的事件时使log Z最小化。这个视图解释了用于描述具有正阶段和负阶段的算法的大多数语言，但是它并没有被神经科学实验证明是正确的。在机器学习模型中，通常需要同时使用正阶段和负阶段，而不是在清醒和REM睡眠的单独时间段中。我们将在章节19.5中看到，其他机器学习算法从模型分布中抽取样本用于其他目的，并且这样的算法还可以解释睡眠中做梦的功能。

鉴于这种对学习的正阶段和负阶段作用的理解，我们可以尝试设计一种比算法18.1更容易计算的替代选择。原始MCMC算法的主要成本是在每一步从随机初始化的马尔科夫链中燃烧的成本。自然的解决方案是从非常接近模型分布的分布中初始化马尔科夫链，使得操作中的燃烧不需要许多步骤。

对比散度（CD或CD-k以指示具有k个Gibbs步长的CD）算法在每个步骤用来自数据分布的样本初始化马可夫链（Hinton，2000,2010）。 这种方法如算法18.2所示。从数据分发中获取样本是免费的，因为它们已经在数据集中可用。最初，数据分布并不接近模型分布，因此负阶段不是非常准确。幸运的是，正阶段仍然可以准确地增加模型的数据概率。在正阶段已经采取行动有一些时间之后，模型分布更接近数据分布，并且负阶段开始变得准确。

当然，CD仍然是正确的负阶段的近似。CD定性地未能实现正确的负阶段的主要原因是，它不能抑制实际远离训练示例的高概率区域。在模型下具有高概率但在数据生成分布下具有低概率的这些区域被称为寄生模式（spurious modes）。
图18.2说明了为什么会发生这种情况。基本上，这是因为模型分布中远离数据分布的模式将不会被在训练点初始化的马尔可夫链访问，除非k非常大。

图18.2：对比散度的负阶段（算法18.2）如何不能抑制杂散模式的图示。杂散模式是存在于模型分布中但在数据分布中不存在的模式。因为对比散度从数据点初始化其马尔可夫链，并且运行马尔可夫链仅仅几个步骤，所以不太可能访问远离数据点的模型中的模式。这意味着当从模型中采样时，我们有时会得到不类似数据的样本。这也意味着，由于在这些模式浪费其一些概率质量，模型会努力将高概率质量放在正确的模式。为了可视化，该图使用稍微简化的距离概念——伪模式远离R中沿着数字线的正确模式。这对应于基于使用R中的单个x变量进行局部移动的马尔科夫链。对于大多数深度概率模型，马尔科夫链是基于吉布斯抽样，可以使单个变量的非局部移动，但不能同时移动所有变量。对于这些问题，通常最好考虑模式之间的编辑距离，而不是欧氏距离。然而，在高维空间中的编辑距离难以在2-D图中描绘。

Carreira-Perpiñan和Hinton（2005）实验表明，CD估计量对于RBM和完全可见的玻尔兹曼机是有偏的，因为它收敛到不同于最大似然估计量的点。他们认为，由于偏差很小，CD可以用作一种较容易的方式来初始化一个模型，稍后可以通过更昂贵的MCMC方法进行微调。Bengio和Delalleau（2009）表明，CD可以解释为丢弃正确MCMC更新梯度的最小条件，这解释了偏差。
CD对于训练浅层模型是有用的，如RBM。这些可以依次堆叠以初始化更深的模型，如DBN或DBM。然而，CD不能为直接训练更深的模型提供很多帮助。这是因为在给定可见单元样本的情况下，难以获得的隐单元的样本。 由于隐单元不包括在数据中，因此从训练点初始化不能解决问题。即使我们从数据初始化可见单位，我们仍然需要在从那些可见样本条件下的隐单元分布中燃烧马尔可夫链取样。

CD算法可以被认为是惩罚具有马尔可夫链的模型，当输入来自数据时，马尔可夫链快速地改变输入。这意味着用CD训练有点类似于自动编码器训练。即使CD比其他一些训练方法更偏倚，它可能有助于预训练稍后将被堆叠的浅模型。这是因为鼓励堆栈中的最早模型将更多信息复制到其隐变量，从而使其可用于稍后的模型。这应该更多地被认为是CD训练的经常可利用的副作用，而不是原则性的设计优点。

Sutskever和Tieleman（2010）表明，CD的更新方向不是任何函数的梯度。这允许CD可以永久循环的情况，但在实践中这不是一个严重的问题。

解决CD的许多问题的一个不同策略是在每个梯度步骤用来自先前梯度步骤的状态初始化马尔科夫链。这种方法首先在应用数学和统计学社群中以随机最大似然（SML）的名称被发现（Younes，1998），随后在深度学习社区（Tieleman，2008）以持续性对比散度（PCD或PCD-k以指示使用k Gibbs 步骤每更新）的名称独立地被重新发现。参见算法18.3。这种方法的基本思想是，只要随机梯度算法采取的步骤很小，则来自前一步骤的模型将类似于来自当前步骤的模型。因此，来自先前模型分布的样本将非常接近于来自当前模型分布的公平样本，因此用这些样本初始化的马尔可夫链将不会需要太多时间来混合。

如果随机梯度算法可以移动模型比马可夫链的混合步骤更快，SML容易变得不准确。如果k太小或e太大，这可能发生。不幸的是，e值的允许范围是高度问题依赖的。没有已知的方式来正式测试链是否在步骤之间成功地混合。主观地，如果学习速率对于吉布斯步数而言太高，则操作人员将能够观察到横跨梯度步骤的负相样本中存在更多的方差，而不是跨越不同的马尔可夫链。例如，在MNIST上训练的模型可以在一个步骤上仅采样7s。 然后，学习过程将强烈地下推对应于7s的模式，并且模型可以在下一步骤中独占地采样9s。

在从SML（随机最大似然）训练的模型中评估样本时必须小心。模型完成训练之后有必要从以随机起点初始化的新马尔可夫链中抽样。持久化负链中用于训练的的样本已经受到该模型的几个最近版本的影响，并且因此可以使该模型看起来具有比其实际上更大的容量。

Berglund和Raiko（2013）进行了实验来检查由CD和SML给出的梯度估计中的偏差和方差。CD被证明具有比基于精确采样的估计器更低的方差。SML具有较高的方差。CD具有低方差的原因是其在正相和负相中使用相同的训练点。如果从不同的训练点初始化负相位，则其方差会在基于精确采样的估计器之上。

所有这些基于使用MCMC从模型中抽样的方法原则上几乎可以与MCMC的任何变体一起使用。这意味着诸如SML的技术可以通过使用第17章中描述的任何增强的MCMC技术来改进，例如并行回火(Desjardins et al., 2010; Cho et al., 2010) 。
有一种方法在学习期间加速混合并不依赖于改变蒙特卡罗采样技术，而是改变模型的参数化和成本函数。快速PCD或FPCD（Tieleman和Hinton，2009）涉及用表达式来替换传统模型的参数θ，

现在有两倍于之前的参数，并且它们一起逐元素地添加由原始模型定义使用的参数。快速参数拷贝以更大的学习率训练，允许其快速响应于学习的负相阶段并且推动马尔科夫链到新的领域。这迫使马尔可夫链迅速混合，虽然这种效应只发生在学习期间，而快速权重是可以自由改变的。 通常，这个权重还对快速权重具有显着的权重衰减，鼓励他们收敛到较小的值，只是瞬时具有足够大的值来刺激马尔可夫链改变模型。

本节中描述的基于MCMC的方法的一个关键优点是它们提供了log Z的梯度的估计，因此我们可以将问题基本上分解为log p的贡献和log Z的贡献。然后我们可以使用任何其他方法来处理log p（x），并且只是将我们的负相位梯度添加到另一个方法的梯度。特别地，这意味着正相可以利用仅在p上提供下界的方法。本章介绍的处理log z的大多数其他方法与基于边界的正相方法不兼容。

###18.3	伪似然（Pseudolikelihood）
蒙特卡罗近似于分区函数且其梯度需要直接处理分区函数。其它方法通过训练模型而不计算分区函数来回避该问题。这些方法中的大多数基于以下观察：在无向概率模型中计算概率比率更容易。 这是因为分区函数出现在比率的分子和分母中，并抵消：

伪似然是基于如下观察，条件概率采用这种基于比率的形式，并因此可以在分区函数未知的情况下计算。假设我们将x分区为a，b和c，其中a包含我们想要找到的条件分布的变量，b包含我们想要条件化的变量，c包含不是查询的一部分的变量。

这个量需要边缘化a，而且这个操作效率非常高，只要a和c不包含非常多的变量。在极端情况下，a可以是单个变量，c可以为空，使得评估p的操作仅需要与单个随机变量的值一样多。
不幸的是，为了计算对数似然，我们需要边际化出大的变量集合。如果有总计n个变量，我们必须边缘化的集合的大小位n-1。根据概率的链式法则，

在这种情况下，我们已经做出了最小的，但c可以大到x2：n。如果我们简单地将c移动到b以减少计算成本？这产生了伪似然（pseudolikelihood，Besag，1975）目标函数，基于在给定所有其他特征x-i的情况下预测特征xi的值：

如果每个随机变量具有k个不同的值，则与计算分区函数所需的kn次评估相反，这仅需要对要计算的P进行k×n次评估。

这看起来可能像一个无原则的黑客，但可以证明通过最大化伪似然的估计是渐近一致的（Mase，1995）。 当然，在数据集不接近大抽样限制的情况下，伪似然可能表现出与最大似然估计不同的行为。

通过使用广义伪似然估计器（Huang和Ogata，2002），有可能和最大似然交换求导的计算复杂度。广义伪似估计器使用m个不同的集合S（i），i = 1，...，m，其中m是出现在调节条左侧的变量的索引。在m = 1和S（1）= 1，...，n的极端情况下，广义伪似然能恢复对数似然。 在m = n和S（i）= {i}的极端情况下，广义伪似然恢复伪似然。广义伪似然目标函数由下式给出，

基于伪似然方法的性能在很大程度上取决于如何使用模型。在需要较好的完全联合概率p（x）的模型的任务（例如密度估计和采样）上，伪似然倾向于表现不佳。然而，对于只需要在训练期间使用的条件分布的任务，例如填充少量的缺失值，它可以比最大似然性更好。如果数据具有规则结构，则允许S索引集被设计为捕获最重要的相关，同时省略仅具有可忽略的相关性的变量组，则广义伪似然技术是特别强大的。例如，在自然图像中，在空间中广泛分离的像素也具有弱相关性，因此可以应用广义伪似然，其中每个S集合是小的空间定位的窗口。

伪似然估计器的一个缺点是它不能与仅在p（x）上提供下界的其他近似一起使用，例如变分推理，这将在第19章中讨论。这是因为p出现在分母中。分母的下界只提供整个表达式的上界，并且没有利益最大化上界。这使得难以将伪似然方法应用于诸如深波兹曼机的深度模型，因为变分方法是将彼此相互作用的许多层隐变量近似边缘化的主要方法之一。 然而，伪似然对于深度学习仍然有用，因为它可以用于训练单层模型或使用不基于下界的近似推理方法的深层模型。
由于其对所有条件的显式计算，伪似然比SML具有大得多的每梯度步长的成本。然而，如果每个例子只计算一个随机选择的条件（Goodfellow等，2013b），则广义伪似然和类似标准仍然可以很好地执行，从而使计算成本降低到与SML匹配。

虽然伪似然估计器没有显式地使logZ最小化，但是它仍然可以被认为具有类似于负相位的东西。每个条件分布的分母导致学习算法抑制具有仅一个变量的所有状态的概率不同于训练示例。
参见Marlin和de Freitas（2011）对伪似然的渐近效率的理论分析。

###18.4	得分匹配和比例匹配（Score Matching and Ratio Matching）
得分匹配（Hyvärinen，2005）提供了另一种一致的方法来训练模型，而不估计Z或其衍生物。得分匹配的名称来自术语，其中对数密度相对于其参数∇xlog p（x）的导数被称为其分数。通过得分匹配使用的策略是最小化相对于输入模型的对数密度的导数与相对于输入数据的对数密度的导数之间的预期平方差。

这个目标函数避免了配分函数Z求导的困难，因为Z不是x的函数，因此∇xZ= 0。最初，得分匹配似乎有一个新的困难：计算数据分布的分数需要知道生成训练数据的真实分布，pdata。幸运的是，最小化L（x，θ）的期望值相当于最小化预期值

其中n是x的维度。

因为得分匹配需要采用关于x的导数，它不适用于离散数据的模型。然而，模型中的隐变量可能是离散的。

像伪似然一样，只有当我们能够直接评估log p（x）及其导数时，得分匹配才有效。它与仅提供logp（x）下限的方法不兼容，因为得分匹配需要log p（x）的导数和二阶导数，而下界不传递关于其导数的信息。这意味着得分匹配不能应用于隐单元之间复杂交互的估计模型，如稀疏编码模型或深波尔兹曼机。虽然得分匹配可以用于预处理较大模型的第一隐层，但是它还没有被应用于较大模型的较深层的预训练策略。这可能是因为这些模型的隐层通常包含一些离散变量。
虽然得分匹配没有明确地具有负相，但可以将其视为使用特定类型的马尔科夫链的对比散度的版本（Hyvärinen，2007a）。在这种情况下，马可夫链不是吉布斯取样，而是使梯度引导局部移动的不同方法。当本地移动的大小接近零时，得分匹配相当于使用这种类型马可夫链的CD。

Lyu（2009）将得分匹配推广到离散的情况（但是由Marlin等（2010）纠正其推导错误）。马林等人（2010）发现，广义得分匹配（GSM，generalized score matching）在许多事件的观察概率为0的高维离散空间中不起作用。

将得分匹配的基本思想扩展到离散数据的更成功的方法是比例匹配（Hyvärinen，2007b）。比例匹配特别适用于二进制数据。比例匹配包括最小化以下目标函数的平均值：

其中f（x，j）返回x，位置j的位被翻转。比例匹配使用与伪似然估计器相同的技巧来避免分区函数：以两个概率的比率，分区函数取消。Marlin et al. (2010) 发现，通过比例匹配训练的模型在去噪声测试集图像上的能力，比例匹配优于SML，伪可能性和GSM。

像伪似然估计器一样，比例匹配需要对每个数据点进行n次评估，使得每次更新的计算成本大大高于SML的N倍。

与伪似然估计器一样，比例匹配可以被认为是将所有幻想状态下降到只有一个变量不同于训练示例的比例。由于比例匹配特别适用于二进制数据，这意味着它对数据中汉明距离1之间的所有幻想状态起作用。

比例匹配也可用作处理高维稀疏数据（如字数向量）的基础。这种数据对基于MCMC的方法构成挑战，因为数据以密集格式表示非常昂贵，但是MCMC采样器在模型已经学会了表示数据分布中的稀疏性之前不会产生稀疏值。Dauphin and Bengio(2013)通过设计比例匹配的无偏随机近似来克服这个问题。近似值仅评估随机选择的目标项的子集，并且不需要模型来生成完整的幻想样本。

参见Marlin和de Freitas（2011），对比例匹配的渐近效率进行的理论分析。

###18.5	去噪得分匹配（Denoising Score Matching）
在某些情况下我们也许希望通过拟合一个分布而不是真实数据来正则化得分匹配。分布q（x | y）是退化过程，通常是通过向y添加少量噪声来形成x的分布。

去噪分数匹配是特别有用的，因为在实践中，通常我们无法访问真实的数据，而只能访问由其中样本定义的经验分布。给予足够的能力的任何一致的估计将使得模型成为以训练点为中心的一组狄拉克分布。平滑q有助于减少这个问题，即在章节5.4.5描述的渐近一致性的损失。 Kingma and LeCun (2010)引入了一种用于执行正则化分数匹配的过程，其中平滑分布q是正态分布的噪声。

从章节14.5.1回忆几个自动编码器训练算法等同于得分匹配或去噪分数匹配。因此，这些自动编码器训练算法是克服配分函数问题的一种方式。

###18.6	对比噪声估计（Noise-Contrastive Estimation）（语言模型中有使用，和Negtive Sampling一起研究）
用于使用难处理的配分函数估计模型的大多数技术不能提供配分函数的估计。SML和CD仅能估计日志配分函数的渐变，而不是配分函数本身。分数匹配和伪似然避免了与配分函数相关的计算量。

噪声对比估计（NCE）（Gutmann和Hyvarinen，2010）采取不同的策略。在这种方法中，由模型估计的概率分布明确表示为

其中c被明确地引入为 - log Z（θ）的近似值。噪声对比估计过程将c仅作为另一个参数，同时使用相同的算法来估计θ和c，而不是仅估计θ。因此，所得到的log pmodel (x)可能不完全对应于有效的概率分布，但随着c的估计改善而变得越来越接近有效。

这种方法不可能使用最大似然值作为估计器的标准。最大似然准则将选择将c任意设置为高，而不是设置c以创建有效的概率分布。

NCE通过将估计p（x）的无监督学习问题简化为学习概率二进制分类器的一个类别对应于模型生成的数据的工作。这种监督学习问题的构建方式是这种监督学习问题中的最大似然估计定义了原始问题的渐近一致估计。

具体来说，我们引入第二个分布，噪声分布pnoise（x）。噪音分布应易于评估和抽样。我们现在可以在x和一个新的二进制类变量y之间构造一个模型。在新的联合模型中，我们指定

和

换句话说，y是一个开关变量，用于确定是否从模型生成x或从噪声分布中生成x。
我们可以构建一个类似的训练数据的联合模型。在这种情况下，开关变量决定是否从数据或噪声分布中抽取x。形式上，ptrain（y = 1）= 1/2，ptrain（x | y = 1）= pdata（x）和ptrain（x | y = 0）= pnoise（x）。

现在我们可以使用标准的最大似然学习来拟合pjoint到ptrain：

分布pjoint本质上是应用于模型的和噪声分布的对数概率差异的逻辑回归模型：


因此，只要log pmodel容易反向传播，并且如上所述，pnoise简单易于评估（为了评估pjoint）和抽样（为了生成训练数据）。

当用于存在少数几个随机变量的问题时，NCE最为成功，但是即使这些随机变量可以承担大量的值，也可以很好地工作。例如，它已被成功地应用于给定上下文（Mnih和Kavukcuoglu，2013）的单词的条件分布建模。虽然这个词可能来自一个大词典，但是只是一个单词。

当NCE应用于存在许多随机变量的问题时，效率就会降低。逻辑回归分类器可以通过识别值不太可能的任何一个变量来拒绝噪声样本。这意味着在pmodel学习了基本的边际统计数据之后，学习速度大大减慢。想象一下，学习面部图像的模型，使用非结构化高斯噪声作为pnoise。如果pmodel了解眼睛，它可以拒绝几乎所有非结构化噪声样本，而不了解任何关于其他面部特征（如嘴巴）的信息。

必须易于评估和容易抽样的约束可能过于严格。当pnoise很简单时，大多数样本很可能与数据明显不同，从而强制pmodel显着改善。

像得分匹配和伪似然一样，如果只有p的下界可用，则NCE不起作用。这样一个下界可以用于构造pjoint（y = 1 | x）的下限，但它只能用于构建pjoint（y = 0 | x）的上界，其出现在NCE目标。同样，pnoise的下界是无用的，因为它只提供pjoint（y = 1 | x）的上界。
当模型分布被复制以在每个梯度步骤之前定义新的噪声分布时，NCE定义了称为自对比估计（self-contrastive estimation）的过程，其预期梯度等于最大似然度的预期梯度（Goodfellow，2014）。NCE的特殊情况，其中噪声样本是由模型产生的噪声样本，表明最大似然可以被解释为强制模型不断学习将现实与其自身不断发展的信念区分开来的过程，而噪声对比估计通过强制模型将现实与固定基线（噪声模型）区分开来实现了计算成本的一些降低。
使用在训练样本和生成样本之间进行分类的监督任务（具有用于定义分类器的模型能量函数）以提供模型上的梯度，早先以各种形式引入（Welling等人，2003b; Bengio，2009）。

噪声对比估计是基于一个好的生成模型应该能够区分数据和噪声的想法。一个密切相关的想法是，良好的生成模型应该能够生成没有分类器可以区分数据的样本。 这个想法产生了生成对抗网络（章节20.10.4）。

###18.7	估计配分函数（Estimating the Partition Function）
虽然本章的大部分内容专门用于描述避免需要计算与无向图模型相关联的难处理分区函数Z（θ）的方法，但在本节中，我们将讨论几种直接估计分区函数的方法。

估计分区函数可能很重要，因为如果我们希望计算数据的归一化可能性，我们需要它。这在评估模型，监控训练过程性能以及相互比较模型方面往往很重要。

假设我们定义两个模型：模型MA定义概率分布pA(x; θA) =1/ZA p A(x;θA)和模型MB定义概率分布pB(x; θB) =1/ZB p B(x;θB)。比较模型的一个常见方式是评估和比较模型为一个i.i.d.测试数据集分配的似然。假设测试集由m个样本{x（1），...，x（m）}组成。如果

或者等效地，如果

那么我们说模型MA优于MB（或者至少在测试集上是一个更好的模型），在某种意义上说它具有更好的测试对数似然性。不幸的是，测试这个条件是否存在需要知道分区函数。不幸的是，方程 18.38似乎需要评估模型分配给每个点的对数概率，这反过来需要评估分区函数。我们可以通过重新排列方程18.38进入一个形式来简化情况，我们只需要知道两个模型的分区函数的比率：

这样我们可以在配分函数未知而仅知道其比率的情况下确定MA是否是优于MB的模型。我们马上会看到，只要这两个模型相似，我们就可以通过重要性采样来估计这个比率。

但是，如果我们想要计算MA或MB下的测试数据的实际概率，则需要计算分区函数的实际值。也就是说，如果我们知道两个分区函数的比率，r = Z（θB）/Z（θA），我们知道两个中只有一个的实际值，即Z（θA），我们可以计算出另外一个的值：

估计分区函数的一种简单方法是使用蒙特卡罗方法，如简单重要性抽样。 我们使用积分来呈现连续变量的方法，但是可以通过用积分替换积分来容易地应用于离散变量。我们使用一个分布p0（x）= 1/Z0 p 0（x），它支持配分函数Z0和非归一化分布p 0（x）都容易采样和易于评估。

在最后一行中，我们使用从p0（x）抽取的样本，得到积分的蒙特卡罗估计量Z1，然后用非归一化p1和p0的比例对每个样本进行加权。我们也看到，这种方法使我们能够估计分区函数之间的比例为

然后这个值就可以直接用于比较这两个模型，如等式18.39所述。

如果分布p0接近p1，等式18.44可以作为估计分区函数的有效方法（Minka，2005）。不幸的是，大多数情况下p1都是复杂的（通常是多模态的）并且被定义在高维空间上。很难找到易于处理的p0，其容易评估，同时仍然足够接近p1以导致高质量近似。如果p0和p1不接近，则p0的大多数样本在p1下具有较低的概率，因此对等式18.44中的和贡献（相对）可以忽略不计。
在这个总和中具有少量具有显着权重的样本将导致由于高方差而质量差的估计器。这可以通过考量我们估计Z1的方差来来量化理解：

当重要性权重的值有显著差异时，这个数量值是最大的。

我们现在转而采用两种被提出的相关策略，以应对在高维空间中估计复杂分布的分区函数的挑战性任务：退火重要性抽样（annealed importance sampling）和桥采样（bridge sampling）。两者都从上面介绍的简单重要性抽样策略入手，并尝试通过引入弥合p0和p1之间的差距的中间分布来克服p0与p1差距太大的问题。

####18.7.1	退火重要性采样（Annealed Importance Sampling）
在DKL（p0||p1）大的情况下（即，在p0和p1之间几乎没有重叠的情况下），称为退火重要性抽样（AIS）的策略试图通过引入中间分布来弥合差距（Jarzynski，1997; Neal，2001）。考虑分布序列pη0，...，pηn，其中0 =η0<η1<... <ηn-1 <ηn= 1，使得序列中的第一个和最后一个分布分别为p0和p1。

这种方法允许我们估计在高维空间（例如经过训练的RBM定义的分布）上定义的多模分布的配分函数。我们从一个更简单的模型开始，具有一个已知的配分函数（比如权重为零的RBM），并估计两个模型的配分函数之间的比例。该比率的估计是基于许多类似分布的序列的比率的估计，例如具有在0和学习到的权重之间插值的权重的RBM序列。

现在我们把比率Z1/Z0写为，

只要分布pηj和pηj+1对于所有0 ≤ j ≤ n – 1都足够接近，我们就可以通过重要性采样可靠地估计每个因子Zηj+1/Zηj，然后再用它们获得Z1/Z0。

这些中间分布来自哪里？正如原始提出的分布p0是设计的选择，分布序列pη1…pηn-1也是一样。也就是说，它可以专门构造来适应问题域。中间分布的一个通用和普遍选择是使用目标分布p1的加权几何平均值和起始提出分布（已知分区函数）p0：

为了从中间分布中抽样，我们定义一系列的马尔科夫链转移方程Tηj(x0 |x)来定义给定当前x时转移到x0的条件概率分布。定义转移算子Tηj(x0 |x)来保留pηj（x）不变：

这些转换可以被构造为任何马尔可夫链蒙特卡罗方法（例如，Metropolis-Hastings，Gibbs），包括涉及多次轮询所有随机变量或其它种类迭代的方法。

AIS抽样策略然后从p0生成样本，然后使用转换算子从中间分布中顺序生成样本，直到从目标分布p1得到样本：

对于样本k，我们可以通过将等式中给出的中间分布之间的跳跃的重要性权重连在一起来导出重要性权重。如等式18.49。

为了避免溢出等计算问题，可能最好在对数空间中计算：

有了采样过程的定义和等式18.52给出的重要性权重，给出配分函数比率的估计如下：

为了验证此过程定义了一个有效的重要性抽样方案，我们可以展示（Neal，2001）AIS过程对应于扩展状态空间上的简单重要性抽样，其中采样点在产品空间[xη1，…，x ηn-1，x1]。为此，我们将扩展空间中的分布定义为：

其中Ta’是由Ta定义的过渡运算符（通过贝叶斯规则的应用）的逆：

将上述等式代入到等式18.56中给出的扩展状态空间的联合分布的表达式中，我们得到：

我们现在有了通过上面给出的抽样方案从扩展样本的联合提出分配q中生成样本的方法，其联合分布由以下方程给出：

我们现在有了由等式18.60给出的扩展空间上的联合分布。将q（xη1，…，x ηn-1，x1）作为我们抽样的扩展状态空间的提出分布，仍然要确定重要性权重：

这些权重与AIS提出的权重相同。因此，我们可以将AIS解释为应用于扩展状态的简单重要性抽样，其有效性接近重要性抽样的有效性。

退火重要性抽样（AIS）首先由Jarzynski（1997）发现，然后再由Neal（2001）独立地发现。目前它是用于估计无向概率模型的分区函数的最常见方法。其原因可能与有影响力论文的出版（Salakhutdinov和Murray，2008）有关，该文章描述了其应用于估计受限玻尔兹曼机和深度信念网络的分区函数，而不是与该方法相比具有任何固有优势的方法。

在Neal（2001）中可以找到关于AIS估计器的性质（例如其方差和效率）的讨论。

####18.7.2	桥采样（Bridge Sampling）
桥采样是另外一种类似于AIS的方法，解决了重要性采样的缺点。桥接采样不是将一系列中间分布链接在一起，而是基于单个分布p\*（称为桥）在已知分区函数p0和我们正在尝试估计其分区函数的分布p1之间插值。

桥采样通过p0和p\*以及p1与p\*间的期望重要性权重来估计比率Z1/Z0。

如果仔细选择的桥分布p*可以使得p0和p1两者具有大的支撑重叠，则桥接采样可以允许两个分布（或更正式地，DKL（p0||p1））之间的距离远大于标准重要性采样。

可以证明最优的桥分布通过下式给出，其中r = Z1/Z0。开始，这似乎是一个不可行的解决方案，因为它似乎需要我们正在估计的数量，Z1/Z0。 然而，可以从r的粗略估计开始，并使用所得到的桥分布来迭代地改进我们的估计（Neal，2005）。也就是说，我们迭代地重新估计比率，并使用每个迭代来更新r的值。

**链式重要性抽样（Linked importance sampling）**。AIS和桥采样都有各自的优点。如果DKL（p0||p1）不是太大（因为p0和p1足够接近），桥采样可以是比AIS更有效的估计分区函数的比率的手段。然而，如果两个分布对于单个分布p*来说弥合差距太大，那么可以至少使用具有许多潜在中间分布的AIS来跨越p0和p1之间的距离。Neal（2005）显示了他提出的链接重要性抽样方法如何利用桥梁采样策略的功能来桥接AIS中使用的中间分布，以显着提高整体的分区函数估计。

**在训练时估算分区函数（Estimating the partition function while training）。**虽然AIS已经被接受为许多无向模型的分区函数估计的标准方法，但它的计算量太大，在训练过程中仍然不可行。然而，已经探索了替代策略来维持训练中分区功能的估计。

使用桥梁取样，短链AIS和平行回火的组合，Desjardins等人（2011）设计了一个在整个训练过程中跟踪RBM分区函数的方案。该策略基于在平行回火方案中运行的每个温度下维持RBM分区函数的独立估计。作者将相邻链的分区函数（即平行回火）的比率的桥采样估计与AIS估计结合在一起，以便在学习的每次迭代时得出分区函数的低方差估计。

本章描述的工具提供了许多不同的方法来克服难处理的分区函数的问题，但在训练和使用生成模型中可能还有其他一些困难。其中最重要的是难以推理的问题，下一步我们将面对这个问题。

